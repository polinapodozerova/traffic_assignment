{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eed1f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.nn import LayerNorm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "594f6838",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_coords = pd.read_csv(\"data/sioux/SiouxFalls_node.tntp\", sep='\\t')\n",
    "node_coords_arr = np.array(node_coords[['X', 'Y']])\n",
    "\n",
    "sioux = create_network_df(network_name=\"SiouxFalls\")\n",
    "T_0, C = prepare_network_data(sioux)\n",
    "\n",
    "directory = \"/home/podozerovapo/traffic_assignment/data/sioux/uncongested\"\n",
    "\n",
    "inputs = []\n",
    "outputs = []\n",
    "metadata = []\n",
    "\n",
    "for filename in sorted(os.listdir(directory)):\n",
    "    if filename.endswith(\".pkl\"):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        \n",
    "        with open(filepath, 'rb') as f:\n",
    "            data_pair = pickle.load(f)\n",
    "            \n",
    "            inputs.append(data_pair['input'])\n",
    "            outputs.append(data_pair['output'])\n",
    "            metadata.append(data_pair.get('metadata', None))\n",
    "\n",
    "input_matrices = np.array(inputs)  # [num_samples, num_nodes, num_nodes]\n",
    "output_matrices = np.array(outputs)  # [num_samples, num_nodes, num_nodes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "17685714",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEmbedding(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size=32):\n",
    "        super(FeatureEmbedding, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, embedding_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class SharedEncoderComponents(nn.Module):\n",
    "    \"\"\"Shared components between V-Encoder and R-Encoder\"\"\"\n",
    "    def __init__(self, node_feat_size, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.node_feat_size = node_feat_size\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = node_feat_size // n_heads\n",
    "        \n",
    "        # Linear transformations for Q, K, V\n",
    "        self.q_linear = nn.Linear(node_feat_size, node_feat_size)\n",
    "        self.k_linear = nn.Linear(node_feat_size, node_feat_size)\n",
    "        self.v_linear = nn.Linear(node_feat_size, node_feat_size)\n",
    "        \n",
    "        # Output transformations\n",
    "        self.out_ffn = nn.Sequential(\n",
    "            nn.Linear(node_feat_size, node_feat_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(node_feat_size, node_feat_size)\n",
    "        )\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(node_feat_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "class VEncoderLayer(nn.Module):\n",
    "    \"\"\"Virtual Link Encoder Layer\"\"\"\n",
    "    def __init__(self, node_feat_size, edge_feat_size, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.shared = SharedEncoderComponents(node_feat_size, n_heads, dropout)\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = node_feat_size // n_heads\n",
    "        # FFN for adaptive virtual edge weights\n",
    "        self.edge_weight_ffn = nn.Sequential(\n",
    "            nn.Linear(2 * node_feat_size, edge_feat_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(edge_feat_size, n_heads),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Additional FFN for the layer\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(node_feat_size, node_feat_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(node_feat_size * 2, node_feat_size),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, adj_mask):\n",
    "        batch_size, num_nodes, _ = x.size()\n",
    "        #print(x.shape) # [64, 24, 3] batch, nodes, \n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        q = self.shared.q_linear(x).view(batch_size, num_nodes, self.n_heads, self.head_dim)\n",
    "        k = self.shared.k_linear(x).view(batch_size, num_nodes, self.n_heads, self.head_dim)\n",
    "        v = self.shared.v_linear(x).view(batch_size, num_nodes, self.n_heads, self.head_dim)\n",
    "        \n",
    "        # Prepare for attention\n",
    "        q = q.transpose(1, 2)  # [batch, heads, nodes, head_dim]\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        \n",
    "        # Compute scaled dot-product attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Compute adaptive edge weights for virtual links\n",
    "        src_nodes = x.unsqueeze(2).expand(-1, -1, num_nodes, -1)\n",
    "        dst_nodes = x.unsqueeze(1).expand(-1, num_nodes, -1, -1)\n",
    "        node_pairs = torch.cat([src_nodes, dst_nodes], dim=-1)\n",
    "        beta = self.edge_weight_ffn(node_pairs).permute(0, 3, 1, 2)  # [batch, heads, nodes, nodes]\n",
    "        \n",
    "        # Apply adjacency mask and combine with attention scores\n",
    "        #print(adj_mask.shape)\n",
    "        adj_mask = adj_mask.unsqueeze(1)  # Add head dimension\n",
    "        #print(scores.shape, adj_mask.shape, beta.shape)\n",
    "        scores = scores * adj_mask * beta  # Apply both mask and adaptive weights\n",
    "        \n",
    "        # Apply softmax\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.shared.dropout(attn_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        output = torch.matmul(attn_weights, v)\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "        output = output.view(batch_size, num_nodes, self.shared.node_feat_size)\n",
    "        \n",
    "        # Residual connection and layer norm\n",
    "        output = self.shared.out_ffn(output)\n",
    "        output = self.shared.dropout(output)\n",
    "        output = self.shared.layer_norm(x + output)\n",
    "        \n",
    "        # FFN block\n",
    "        ffn_output = self.ffn(output)\n",
    "        output = self.shared.layer_norm(output + ffn_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class REncoderLayer(nn.Module):\n",
    "    \"\"\"Real Link Encoder Layer\"\"\"\n",
    "    def __init__(self, node_feat_size, edge_feat_size, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        #print('node feat size', node_feat_size)\n",
    "        self.shared = SharedEncoderComponents(node_feat_size, n_heads, dropout)\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = node_feat_size // n_heads\n",
    "        #print(edge_feat_size)\n",
    "        # Edge feature transformation\n",
    "        self.edge_feat_transform = nn.Linear(edge_feat_size, n_heads)\n",
    "        \n",
    "        # Additional FFN for the layer\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(node_feat_size, node_feat_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(node_feat_size * 2, node_feat_size),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, adj_mask, edge_features):\n",
    "        batch_size, num_nodes, _ = x.size()\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        q = self.shared.q_linear(x).view(batch_size, num_nodes, self.n_heads, self.head_dim)\n",
    "        k = self.shared.k_linear(x).view(batch_size, num_nodes, self.n_heads, self.head_dim)\n",
    "        v = self.shared.v_linear(x).view(batch_size, num_nodes, self.n_heads, self.head_dim)\n",
    "        \n",
    "        # Prepare for attention\n",
    "        q = q.transpose(1, 2)  # [batch, heads, nodes, head_dim]\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        \n",
    "        # Compute scaled dot-product attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Transform edge features\n",
    "        edge_weights = self.edge_feat_transform(edge_features).permute(0, 3, 1, 2)  # [batch, heads, nodes, nodes]\n",
    "        \n",
    "        # Apply adjacency mask and edge features\n",
    "        adj_mask = adj_mask.unsqueeze(1)  # Add head dimension\n",
    "        scores = scores * adj_mask * torch.sigmoid(edge_weights)  # Combine with edge features\n",
    "        \n",
    "        # Apply softmax\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.shared.dropout(attn_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        output = torch.matmul(attn_weights, v)\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "        output = output.view(batch_size, num_nodes, self.shared.node_feat_size)\n",
    "        \n",
    "        # Residual connection and layer norm\n",
    "        output = self.shared.out_ffn(output)\n",
    "        output = self.shared.dropout(output)\n",
    "        output = self.shared.layer_norm(x + output)\n",
    "        \n",
    "        # FFN block\n",
    "        ffn_output = self.ffn(output)\n",
    "        output = self.shared.layer_norm(output + ffn_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class VEncoder(nn.Module):\n",
    "    \"\"\"Stacked Virtual Link Encoder\"\"\"\n",
    "    def __init__(self, node_feat_size, edge_feat_size, n_layers=3, n_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            VEncoderLayer(node_feat_size, edge_feat_size, n_heads, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x, v_adj_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, v_adj_mask)\n",
    "        return x\n",
    "\n",
    "class REncoder(nn.Module):\n",
    "    \"\"\"Stacked Real Link Encoder\"\"\"\n",
    "    def __init__(self, node_feat_size, edge_feat_size, n_layers=3, n_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            REncoderLayer(node_feat_size, edge_feat_size, n_heads, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x, adj_mask, edge_features):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, adj_mask, edge_features)\n",
    "        return x\n",
    "\n",
    "class DualGraphEncoder(nn.Module):\n",
    "    \"\"\"Complete architecture with both V-Encoder and R-Encoder\"\"\"\n",
    "    def __init__(self, node_feat_size, edge_feat_size,\n",
    "                 v_layers=3, r_layers=3, n_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.vencoder = VEncoder(node_feat_size, edge_feat_size, v_layers, n_heads, dropout)\n",
    "        self.rencoder = REncoder(node_feat_size, edge_feat_size, r_layers, n_heads, dropout)\n",
    "\n",
    "    def forward(self, node_features, real_adj_mask, virtual_adj_mask, edge_features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            node_features: Input node features [batch, nodes, feat_size]\n",
    "            real_adj_mask: Adjacency mask for real edges [batch, nodes, nodes]\n",
    "            edge_features: Edge features for real links [batch, nodes, nodes, edge_feat_size]\n",
    "            virtual_adj_mask: Mask for virtual edges [batch, nodes, nodes]\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (v_encoded, r_encoded) features\n",
    "        \"\"\"\n",
    "        batch_size, num_nodes, _ = node_features.size()\n",
    "        \n",
    "        # Create virtual adjacency mask if not provided (self-loops only)\n",
    "        if virtual_adj_mask is None:\n",
    "            raise ValueError(\"no virtual adjacency mask provided\")\n",
    "        \n",
    "        # Combine real and virtual edges for V-Encoder\n",
    "        # v_adj_mask = real_adj_mask + virtual_adj_mask\n",
    "        # v_adj_mask = (v_adj_mask > 0).float()\n",
    "        \n",
    "        # Process through V-Encoder (captures long-range dependencies)\n",
    "        v_encoded = self.vencoder(node_features, virtual_adj_mask)\n",
    "        \n",
    "        # Process through R-Encoder (captures local topology)\n",
    "        r_encoded = self.rencoder(node_features, real_adj_mask, edge_features)\n",
    "        \n",
    "        return v_encoded, r_encoded\n",
    "\n",
    "class TrafficAssignmentModel(nn.Module):\n",
    "    \"\"\"Complete model with preprocessing, dual encoders, and flow prediction\"\"\"\n",
    "    def __init__(self, num_nodes, node_feat_size=32, edge_feat_size=2,\n",
    "                 v_layers=2, r_layers=2, n_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.feature_preprocessor = FeatureEmbedding(input_size=num_nodes + 2, embedding_size=32)\n",
    "        \n",
    "        # Feature preprocessing would be done separately\n",
    "        self.dual_encoder = DualGraphEncoder(node_feat_size, edge_feat_size, \n",
    "                                           v_layers, r_layers, n_heads, dropout)\n",
    "        \n",
    "        # Flow prediction head\n",
    "        self.flow_predictor = nn.Sequential(\n",
    "            nn.Linear(4 * node_feat_size, node_feat_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(node_feat_size, 1),\n",
    "            nn.ReLU()  # Flow must be non-negative\n",
    "        )\n",
    "\n",
    "    def forward(self, node_features, real_adj_mask, virt_adj_mask, edge_features):\n",
    "        \n",
    "        node_features = self.feature_preprocessor(node_features)\n",
    "        #print('!!!!!!', real_adj_mask.shape)\n",
    "        v_encoded, r_encoded = self.dual_encoder(node_features, real_adj_mask, virt_adj_mask, edge_features)\n",
    "        #print(v_encoded.shape, r_encoded.shape)\n",
    "        combined = torch.cat([v_encoded, r_encoded], dim=-1)\n",
    "        src_nodes = combined.unsqueeze(2).expand(-1, -1, combined.size(1), -1)\n",
    "        dst_nodes = combined.unsqueeze(1).expand(-1, combined.size(1), -1, -1)\n",
    "        #print(src_nodes.shape, dst_nodes.shape)\n",
    "        pair_features = torch.cat([src_nodes, dst_nodes], dim=-1)\n",
    "        #print(pair_features.shape)\n",
    "        \n",
    "        flows = self.flow_predictor(pair_features).squeeze(-1) # [batch, nodes, nodes, 1]\n",
    "        flows = flows * real_adj_mask\n",
    "        \n",
    "        return flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fede244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_for_concat = np.repeat(np.expand_dims(node_coords_arr, 0), repeats=input_matrices.shape[0],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6efc6ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4096, 24, 26)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.concatenate([input_matrices, coords_for_concat], axis=2)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0875fd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "capacities = np.repeat(np.expand_dims(C, 0), axis=0, repeats=input_matrices.shape[0])\n",
    "free_flow_times = np.repeat(np.expand_dims(T_0, 0), axis=0, repeats=input_matrices.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ed88027",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficDataset(Dataset):\n",
    "    def __init__(self, X_data, capacity_data, free_flow_data, flows_data):\n",
    "        self.data = []\n",
    "        \n",
    "        # Нормализация данных\n",
    "        for x, cap, fft, fl in zip(X_data, capacity_data, free_flow_data, flows_data):\n",
    "            # Отделяем OD-матрицу (спрос) и координаты\n",
    "            od_matrix = x[:, :-2]\n",
    "            coordinates = x[:, -2:]\n",
    "            \n",
    "            # Нормализация спроса\n",
    "            od_mean = od_matrix.mean()\n",
    "            od_std = od_matrix.std() + 1e-8\n",
    "            od_matrix = (od_matrix - od_mean) / od_std\n",
    "    \n",
    "            # Нормализация координат\n",
    "            coord_mean = coordinates.mean(axis=0)\n",
    "            coord_std = coordinates.std(axis=0) + 1e-8\n",
    "            coordinates = (coordinates - coord_mean) / coord_std\n",
    "    \n",
    "            x_normalized = np.concatenate([od_matrix, coordinates], axis=1)\n",
    "            \n",
    "            # Нормализация capacity и free_flow_time\n",
    "            cap = (cap - cap.mean()) / (cap.std() + 1e-8)\n",
    "            fft = (fft - fft.mean()) / (fft.std() + 1e-8)\n",
    "            \n",
    "            # Нормализация целевых потоков\n",
    "            fl = (fl - fl.mean()) / (fl.std() + 1e-8)\n",
    "            \n",
    "            self.data.append({\n",
    "                'od_matrix': torch.FloatTensor(od_matrix),\n",
    "                'coordinates': torch.FloatTensor(coordinates),\n",
    "                'capacity': torch.FloatTensor(cap),\n",
    "                'free_flow': torch.FloatTensor(fft),\n",
    "                'flows': torch.FloatTensor(fl),\n",
    "                'x_normalized' : torch.FloatTensor(x_normalized)\n",
    "            })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        # Создание масок с учетом величины спроса\n",
    "        real_adj_mask = (item['capacity'] > 0).float()\n",
    "        virtual_adj = item['od_matrix'].abs()  # Учитываем абсолютное значение спроса\n",
    "        \n",
    "        # Объединение признаков ребер\n",
    "        edge_features = torch.stack([\n",
    "            item['capacity'],\n",
    "            item['free_flow']\n",
    "        ], dim=-1)\n",
    "        \n",
    "        # Нормализованные node features = координаты + агрегированный спрос\n",
    "        agg_demand = item['od_matrix'].sum(dim=1, keepdim=True)\n",
    "        node_features = torch.cat([item['coordinates'], agg_demand], dim=1)\n",
    "        \n",
    "        return {\n",
    "            'node_features': node_features,\n",
    "            'real_adj_mask': real_adj_mask,\n",
    "            'edge_features': edge_features,\n",
    "            'virtual_adj': virtual_adj,\n",
    "            'target_flows': item['flows'],\n",
    "            'x_normalized' : item['x_normalized']\n",
    "        }\n",
    "\n",
    "# class TrafficAssignmentModel(nn.Module):\n",
    "#     def __init__(self, node_feat_size=32, edge_feat_size=2,\n",
    "#                  v_layers=2, r_layers=2, n_heads=4, dropout=0.1):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         # Нормализация входных данных\n",
    "#         self.node_bn = LayerNorm(3)  # координаты(2) + агрегированный спрос(1)\n",
    "#         self.edge_bn = LayerNorm(edge_feat_size)\n",
    "        \n",
    "#         # Кодировщики\n",
    "#         self.dual_encoder = DualGraphEncoder(\n",
    "#             node_feat_size, edge_feat_size, v_layers, r_layers, n_heads, dropout\n",
    "#         )\n",
    "        \n",
    "#         # Предиктор потоков с учетом спроса\n",
    "#         self.flow_predictor = nn.Sequential(\n",
    "#             nn.Linear(2*node_feat_size + 1, node_feat_size),  # +1 для спроса\n",
    "#             nn.ReLU(),\n",
    "#             LayerNorm(node_feat_size),\n",
    "#             nn.Linear(node_feat_size, 1),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, node_features, real_adj_mask, edge_features, virtual_adj):\n",
    "#         # Нормализация\n",
    "#         node_features = self.node_bn(node_features)\n",
    "#         edge_features = self.edge_bn(edge_features)\n",
    "        \n",
    "#         # Кодирование\n",
    "#         v_encoded, r_encoded = self.dual_encoder(\n",
    "#             node_features, real_adj_mask, edge_features, virtual_adj\n",
    "#         )\n",
    "        \n",
    "#         # Комбинирование признаков с добавлением спроса\n",
    "#         batch_size, n_nodes, _ = v_encoded.size()\n",
    "#         src = v_encoded.unsqueeze(2) + r_encoded.unsqueeze(2)\n",
    "#         dst = v_encoded.unsqueeze(1) + r_encoded.unsqueeze(1)\n",
    "        \n",
    "#         # Добавляем информацию о спросе к парам узлов\n",
    "#         demand = virtual_adj.unsqueeze(-1)  # [batch, n, n, 1]\n",
    "#         pair_features = torch.cat([\n",
    "#             src.expand(-1,-1,n_nodes,-1),\n",
    "#             dst.expand(-1,n_nodes,-1,-1),\n",
    "#             demand\n",
    "#         ], dim=-1)\n",
    "        \n",
    "#         # Прогнозирование потоков\n",
    "#         flows = self.flow_predictor(pair_features).squeeze(-1)\n",
    "#         return flows * real_adj_mask\n",
    "\n",
    "# class VEncoderLayer(nn.Module):\n",
    "#     \"\"\"Virtual Link Encoder Layer\"\"\"\n",
    "#     def __init__(self, node_feat_size, edge_feat_size, n_heads, dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         self.shared = SharedEncoderComponents(node_feat_size, n_heads, dropout)\n",
    "        \n",
    "#         # FFN for adaptive virtual edge weights\n",
    "#         self.edge_weight_ffn = nn.Sequential(\n",
    "#             nn.Linear(2 * node_feat_size, edge_feat_size),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(edge_feat_size, n_heads),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "        \n",
    "#         # Additional FFN for the layer\n",
    "#         self.ffn = nn.Sequential(\n",
    "#             nn.Linear(node_feat_size, node_feat_size * 2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(node_feat_size * 2, node_feat_size),\n",
    "#             nn.Dropout(dropout)\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x, adj_mask, demand):\n",
    "#         batch_size, num_nodes, _ = x.size()\n",
    "        \n",
    "#         # Project to Q, K, V\n",
    "#         q = self.shared.q_linear(x).view(batch_size, num_nodes, self.n_heads, self.head_dim)\n",
    "#         k = self.shared.k_linear(x).view(batch_size, num_nodes, self.n_heads, self.head_dim)\n",
    "#         v = self.shared.v_linear(x).view(batch_size, num_nodes, self.n_heads, self.head_dim)\n",
    "        \n",
    "#         # Prepare for attention\n",
    "#         q = q.transpose(1, 2)  # [batch, heads, nodes, head_dim]\n",
    "#         k = k.transpose(1, 2)\n",
    "#         v = v.transpose(1, 2)\n",
    "        \n",
    "#         # Compute scaled dot-product attention\n",
    "#         scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "#         # Compute adaptive edge weights for virtual links\n",
    "#         src_demand = demand.unsqueeze(-1)\n",
    "#         dst_demand = demand.unsqueeze(-2)\n",
    "#         demand_pairs = torch.cat([src_demand, dst_demand], dim=-1)\n",
    "        \n",
    "#         # Модифицированное вычисление beta с учетом спроса\n",
    "#         node_pairs = torch.cat([\n",
    "#             x.unsqueeze(2).expand(-1,-1,num_nodes,-1),\n",
    "#             x.unsqueeze(1).expand(-1,num_nodes,-1,-1),\n",
    "#             demand_pairs\n",
    "#         ], dim=-1)\n",
    "        \n",
    "#         beta = self.edge_weight_ffn(node_pairs).permute(0,3,1,2) # [batch, heads, nodes, nodes]\n",
    "#         # Apply adjacency mask and combine with attention scores\n",
    "#         adj_mask = adj_mask.unsqueeze(1)  # Add head dimension\n",
    "#         scores = scores * adj_mask * beta  # Apply both mask and adaptive weights\n",
    "        \n",
    "#         # Apply softmax\n",
    "#         attn_weights = F.softmax(scores, dim=-1)\n",
    "#         attn_weights = self.shared.dropout(attn_weights)\n",
    "        \n",
    "#         # Apply attention to values\n",
    "#         output = torch.matmul(attn_weights, v)\n",
    "#         output = output.transpose(1, 2).contiguous()\n",
    "#         output = output.view(batch_size, num_nodes, self.shared.node_feat_size)\n",
    "        \n",
    "#         # Residual connection and layer norm\n",
    "#         output = self.shared.out_ffn(output)\n",
    "#         output = self.shared.dropout(output)\n",
    "#         output = self.shared.layer_norm(x + output)\n",
    "        \n",
    "#         # FFN block\n",
    "#         ffn_output = self.ffn(output)\n",
    "#         output = self.shared.layer_norm(output + ffn_output)\n",
    "        \n",
    "#         return output\n",
    "\n",
    "# Пример использования с нормализацией и учетом спроса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "add15e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "eaaf3d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=100, lr=1e-3, patience=5):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=3)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        #print('train')\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            # Prepare batch data\n",
    "            # node_feat = batch['node_features'].to(device)\n",
    "            node_feat = batch['x_normalized'].to(device)\n",
    "            real_mask = batch['real_adj_mask'].to(device)\n",
    "            edge_feat = batch['edge_features'].to(device)\n",
    "            virt_adj = batch['virtual_adj'].to(device)\n",
    "            targets = batch['target_flows'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(node_feat, real_mask, virt_adj, edge_feat)\n",
    "            \n",
    "            # Compute loss only on real edges\n",
    "            loss = criterion(outputs[real_mask.bool()], targets[real_mask.bool()])\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * node_feat.size(0)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            #print('val')\n",
    "            for batch in val_loader:\n",
    "                #node_feat = batch['node_features'].to(device)\n",
    "                node_feat = batch['x_normalized'].to(device)\n",
    "                real_mask = batch['real_adj_mask'].to(device)\n",
    "                edge_feat = batch['edge_features'].to(device)\n",
    "                virt_adj = batch['virtual_adj'].to(device)\n",
    "                targets = batch['target_flows'].to(device)\n",
    "                \n",
    "                outputs = model(node_feat, real_mask, virt_adj, edge_feat)\n",
    "                \n",
    "                loss = criterion(outputs[real_mask.bool()], targets[real_mask.bool()])\n",
    "                val_loss += loss.item() * node_feat.size(0)\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        train_history.append(train_loss)\n",
    "        val_history.append(val_loss)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        print(f'Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')\n",
    "        print(f'LR: {optimizer.param_groups[0][\"lr\"]:.2e}')\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == patience:\n",
    "                print(f'Early stopping after {epoch+1} epochs')\n",
    "                break\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_history, label='Train Loss')\n",
    "    plt.plot(val_history, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training History')\n",
    "    plt.legend()\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a5a06c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(model, data_loader, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc='Making predictions'):\n",
    "            node_feat = batch['x_normalized'].to(device)\n",
    "            real_mask = batch['real_adj_mask'].to(device)\n",
    "            edge_feat = batch['edge_features'].to(device)\n",
    "            virt_adj = batch['virtual_adj'].to(device)\n",
    "            targets = batch['target_flows'].to(device)\n",
    "            \n",
    "            outputs = model(node_feat, real_mask, virt_adj, edge_feat)\n",
    "            \n",
    "            mask = real_mask.bool()\n",
    "            all_predictions.append(outputs[mask].cpu().numpy())\n",
    "            all_targets.append(targets[mask].cpu().numpy())\n",
    "            break\n",
    "    \n",
    "    predictions = np.concatenate(all_predictions)\n",
    "    targets = np.concatenate(all_targets)\n",
    "    \n",
    "    return predictions, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6a055b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making predictions:   0%|          | 0/52 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "ans = model_predict(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d2e2e274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.52857995 1.0203347  0.5007929  ... 2.244346   2.7004042  0.9136704 ]\n",
      "[0.51010585 1.0598823  0.5754481  ... 2.2259939  2.9019318  0.9558847 ]\n"
     ]
    }
   ],
   "source": [
    "print(ans[0], ans[1], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:03<05:46,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Train Loss: 1.7905 | Val Loss: 0.8613\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:06<05:42,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "Train Loss: 0.6359 | Val Loss: 0.3121\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [00:10<05:47,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100\n",
      "Train Loss: 0.1756 | Val Loss: 0.0997\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [00:14<05:37,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n",
      "Train Loss: 0.0990 | Val Loss: 0.0806\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [00:17<05:43,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "Train Loss: 0.0864 | Val Loss: 0.0763\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [00:21<05:30,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "Train Loss: 0.0797 | Val Loss: 0.0718\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [00:24<05:26,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100\n",
      "Train Loss: 0.0746 | Val Loss: 0.0678\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [00:28<05:17,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100\n",
      "Train Loss: 0.0713 | Val Loss: 0.0669\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [00:31<05:20,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100\n",
      "Train Loss: 0.0688 | Val Loss: 0.0634\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [00:35<05:19,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100\n",
      "Train Loss: 0.0664 | Val Loss: 0.0617\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [00:39<05:19,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100\n",
      "Train Loss: 0.0646 | Val Loss: 0.0603\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [00:42<05:16,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100\n",
      "Train Loss: 0.0628 | Val Loss: 0.0582\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [00:45<05:06,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100\n",
      "Train Loss: 0.0608 | Val Loss: 0.0587\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [00:49<05:02,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100\n",
      "Train Loss: 0.0590 | Val Loss: 0.0551\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [00:53<05:00,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100\n",
      "Train Loss: 0.0580 | Val Loss: 0.0545\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [00:56<04:47,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100\n",
      "Train Loss: 0.0561 | Val Loss: 0.0521\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/100 [00:59<04:36,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100\n",
      "Train Loss: 0.0546 | Val Loss: 0.0526\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18/100 [01:02<04:29,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100\n",
      "Train Loss: 0.0537 | Val Loss: 0.0502\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19/100 [01:04<04:03,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100\n",
      "Train Loss: 0.0519 | Val Loss: 0.0496\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [01:07<03:50,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100\n",
      "Train Loss: 0.0510 | Val Loss: 0.0504\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [01:10<04:01,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100\n",
      "Train Loss: 0.0507 | Val Loss: 0.0478\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22/100 [01:14<04:07,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100\n",
      "Train Loss: 0.0500 | Val Loss: 0.0481\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 23/100 [01:17<04:05,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100\n",
      "Train Loss: 0.0482 | Val Loss: 0.0468\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24/100 [01:21<04:09,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100\n",
      "Train Loss: 0.0475 | Val Loss: 0.0462\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25/100 [01:24<04:09,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100\n",
      "Train Loss: 0.0468 | Val Loss: 0.0456\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 26/100 [01:28<04:11,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100\n",
      "Train Loss: 0.0468 | Val Loss: 0.0438\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 27/100 [01:31<04:09,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100\n",
      "Train Loss: 0.0460 | Val Loss: 0.0451\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 28/100 [01:35<04:10,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100\n",
      "Train Loss: 0.0453 | Val Loss: 0.0444\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 29/100 [01:38<04:08,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100\n",
      "Train Loss: 0.0449 | Val Loss: 0.0425\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [01:42<04:07,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100\n",
      "Train Loss: 0.0443 | Val Loss: 0.0430\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [01:45<04:03,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100\n",
      "Train Loss: 0.0440 | Val Loss: 0.0420\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32/100 [01:49<04:04,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100\n",
      "Train Loss: 0.0437 | Val Loss: 0.0427\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 33/100 [01:53<04:01,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100\n",
      "Train Loss: 0.0434 | Val Loss: 0.0413\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 34/100 [01:56<03:52,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100\n",
      "Train Loss: 0.0422 | Val Loss: 0.0426\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35/100 [02:00<03:47,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/100\n",
      "Train Loss: 0.0419 | Val Loss: 0.0397\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 36/100 [02:03<03:43,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/100\n",
      "Train Loss: 0.0415 | Val Loss: 0.0405\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 37/100 [02:06<03:39,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/100\n",
      "Train Loss: 0.0409 | Val Loss: 0.0390\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 38/100 [02:10<03:37,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/100\n",
      "Train Loss: 0.0404 | Val Loss: 0.0405\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 39/100 [02:14<03:33,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/100\n",
      "Train Loss: 0.0397 | Val Loss: 0.0395\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [02:17<03:31,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/100\n",
      "Train Loss: 0.0396 | Val Loss: 0.0379\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 41/100 [02:20<03:24,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/100\n",
      "Train Loss: 0.0391 | Val Loss: 0.0402\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42/100 [02:24<03:19,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/100\n",
      "Train Loss: 0.0389 | Val Loss: 0.0383\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 43/100 [02:27<03:15,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/100\n",
      "Train Loss: 0.0382 | Val Loss: 0.0362\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44/100 [02:30<03:08,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/100\n",
      "Train Loss: 0.0376 | Val Loss: 0.0365\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 45/100 [02:34<03:07,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100\n",
      "Train Loss: 0.0376 | Val Loss: 0.0359\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 46/100 [02:37<03:06,  3.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/100\n",
      "Train Loss: 0.0368 | Val Loss: 0.0377\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 47/100 [02:41<03:05,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/100\n",
      "Train Loss: 0.0367 | Val Loss: 0.0353\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 48/100 [02:45<03:01,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/100\n",
      "Train Loss: 0.0364 | Val Loss: 0.0361\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 49/100 [02:48<02:59,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100\n",
      "Train Loss: 0.0362 | Val Loss: 0.0353\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [02:52<02:55,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100\n",
      "Train Loss: 0.0362 | Val Loss: 0.0345\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 51/100 [02:55<02:51,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100\n",
      "Train Loss: 0.0360 | Val Loss: 0.0367\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 52/100 [02:59<02:48,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/100\n",
      "Train Loss: 0.0360 | Val Loss: 0.0340\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 53/100 [03:02<02:43,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/100\n",
      "Train Loss: 0.0350 | Val Loss: 0.0337\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 54/100 [03:06<02:41,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/100\n",
      "Train Loss: 0.0350 | Val Loss: 0.0337\n",
      "LR: 1.00e-03\n"
     ]
    }
   ],
   "source": [
    "n_nodes = 24\n",
    "X_data = X\n",
    "capacity_data = capacities\n",
    "free_flow_data = free_flow_times\n",
    "flows_data = output_matrices\n",
    "\n",
    "full_dataset = TrafficDataset(X_data, capacity_data, free_flow_data, flows_data)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "model = TrafficAssignmentModel(24)\n",
    "\n",
    "# Запуск обучения\n",
    "trained_model = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs=100,\n",
    "    lr=1e-3,\n",
    "    patience=10\n",
    ")\n",
    "\n",
    "# Сохранение финальной модели\n",
    "torch.save(trained_model.state_dict(), 'final_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ca1918b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4160.0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "104*64 / 2 / 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3b89ee1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"\"\"torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([64, 4, 24, 24]) torch.Size([64, 1, 24, 24]) torch.Size([64, 4, 24, 24])\n",
    "torch.Size([12, 4, 24, 24]) torch.Size([12, 1, 24, 24]) torch.Size([12, 4, 24, 24])\n",
    "torch.Size([12, 4, 24, 24]) torch.Size([12, 1, 24, 24]) torch.Size([12, 4, 24, 24])\"\"\"\n",
    "s.count('torch') / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0187fc97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "36864 / 64 / 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6abf1892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "49152 / 64 / 24 / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39833772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "768 / 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e095569f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7c8dcadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv  # Using PyG for optimized GNN operations\n",
    "\n",
    "class FlowDistributionModel(nn.Module):\n",
    "    def __init__(self, num_nodes, hidden_dim=128, num_heads=2):\n",
    "        super().__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Input feature encoder (3 features: OD, C, T0)\n",
    "        # self.encoder = nn.Sequential(\n",
    "        #     nn.Linear(3, hidden_dim),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(hidden_dim, hidden_dim)\n",
    "        #     #nn.Identity()\n",
    "        # ).to(self.device)\n",
    "        \n",
    "        # Graph Attention Layers (using PyG for better GPU utilization)\n",
    "        self.gat1 = GATConv(3, 16, heads=num_heads).to(self.device)\n",
    "        self.gat2 = GATConv(16, 16, heads=num_heads).to(self.device)\n",
    "        self.gat3 = GATConv(32, 1, heads=1).to(self.device)\n",
    "        \n",
    "        # Output decoder\n",
    "        # self.decoder = nn.Sequential(\n",
    "        #     nn.Linear(hidden_dim, hidden_dim),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(hidden_dim, 1)\n",
    "        # ).to(self.device)\n",
    "        \n",
    "        # Prepare static edge index for complete graph\n",
    "        self.edge_index = self._create_complete_graph_edge_index(num_nodes).to(self.device)\n",
    "    \n",
    "    def _create_complete_graph_edge_index(self, num_nodes):\n",
    "        \"\"\"Create edge index for fully connected graph\"\"\"\n",
    "        adj = torch.ones(num_nodes, num_nodes) - torch.eye(num_nodes)\n",
    "        edge_index = adj.nonzero(as_tuple=False).t()  # Get indices and transpose\n",
    "        return edge_index\n",
    "    \n",
    "    def forward(self, OD, C, T0):\n",
    "        batch_size = OD.shape[0]\n",
    "        \n",
    "        # Move inputs to GPU\n",
    "        OD, C, T0 = OD.to(self.device), C.to(self.device), T0.to(self.device)\n",
    "        \n",
    "        # Prepare edge features [batch_size * num_edges, 3]\n",
    "        edge_features = torch.stack([OD, C, T0], dim=-1)  # [batch, N, N, 3]\n",
    "        edge_features = edge_features.view(-1, 3)  # [batch*N*N, 3]\n",
    "        \n",
    "        x = edge_features\n",
    "        \n",
    "        # Process each graph in batch separately (using vmap would be better)\n",
    "        outputs = []\n",
    "        for i in range(batch_size):\n",
    "            # Get features for this graph\n",
    "            start_idx = i * self.num_nodes * self.num_nodes\n",
    "            end_idx = (i+1) * self.num_nodes * self.num_nodes\n",
    "            x_graph = x[start_idx:end_idx][self.edge_index[0] * self.num_nodes + self.edge_index[1]]\n",
    "            \n",
    "            # GAT layers\n",
    "            h = F.softplus(self.gat1(x_graph, self.edge_index))\n",
    "            h = F.softplus(self.gat2(x_graph, self.edge_index))\n",
    "            h = F.softplus(self.gat3(h, self.edge_index))\n",
    "            \n",
    "            # Decode to flows\n",
    "            flows = torch.abs(h.squeeze(-1))\n",
    "            \n",
    "            # Reshape to adjacency matrix\n",
    "            flow_mat = torch.zeros(self.num_nodes, self.num_nodes, device=self.device)\n",
    "            flow_mat[self.edge_index[0], self.edge_index[1]] = flows\n",
    "            outputs.append(flow_mat)\n",
    "        \n",
    "        return torch.stack(outputs)  # [batch, N, N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819533d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a069727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(pred_flows, target_flows, OD, C):\n",
    "    # Основная MSE потеря\n",
    "    mse_loss = F.mse_loss(pred_flows, target_flows)\n",
    "    \n",
    "    # Потеря на сохранение потока (входной поток = выходной)\n",
    "    inflow = pred_flows.sum(dim=1)  # [batch, num_nodes]\n",
    "    outflow = pred_flows.sum(dim=2)  # [batch, num_nodes]\n",
    "    flow_conservation_loss = F.mse_loss(inflow, outflow)\n",
    "    \n",
    "    # Потеря на превышение пропускной способности\n",
    "    capacity_violation = torch.relu(pred_flows - C).mean()\n",
    "    \n",
    "    # Комбинированная потеря\n",
    "    total_loss = mse_loss + 2 * flow_conservation_loss + 2 * capacity_violation\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f39ef207",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for traffic assignment problem\n",
    "    \"\"\"\n",
    "    def __init__(self, od_matrices, capacities, free_flow_times, link_flows):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            od_matrices: List/array of OD demand matrices [num_samples, num_nodes, num_nodes]\n",
    "            capacities: List/array of capacity matrices [num_samples, num_edges]\n",
    "            free_flow_times: List/array of free-flow time matrices [num_samples, num_nodes, num_nodes]\n",
    "            link_flows: List/array of ground truth link flows [num_samples, num_edges]\n",
    "        \"\"\"\n",
    "        self.nodes_num = len(od_matrices)\n",
    "        self.od_matrices = torch.tensor(od_matrices, dtype=torch.float32)\n",
    "        self.capacities = torch.tensor([capacities for _ in range(self.nodes_num)], dtype=torch.float32)\n",
    "        self.free_flow_times = torch.tensor([free_flow_times for _ in range(self.nodes_num)], dtype=torch.float32)\n",
    "        self.link_flows = torch.tensor(link_flows, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.od_matrices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'od_matrix': self.od_matrices[idx],\n",
    "            'capacity': self.capacities[idx],\n",
    "            'free_flow_time': self.free_flow_times[idx],\n",
    "            'link_flows': self.link_flows[idx]\n",
    "        }\n",
    "    \n",
    "    def get_batches(self, batch_size):\n",
    "        dataloader = torch.utils.data.DataLoader(\n",
    "            self, batch_size=batch_size, shuffle=True\n",
    "        )\n",
    "        return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ea7563c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4096 samples\n",
      "Input shape: (4096, 24, 24)\n",
      "Output shape: (4096, 24, 24)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TrafficDataset' object has no attribute 'get_batches'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_matrices\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m dataset \u001b[38;5;241m=\u001b[39m TrafficDataset(input_matrices, C, T_0, output_matrices)\n\u001b[0;32m---> 29\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batches\u001b[49m(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[1;32m     30\u001b[0m num_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mod_matrix\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     31\u001b[0m num_edges \u001b[38;5;241m=\u001b[39m (dataset[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcapacity\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39msum()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TrafficDataset' object has no attribute 'get_batches'"
     ]
    }
   ],
   "source": [
    "sioux = create_network_df(network_name=\"SiouxFalls\")\n",
    "T_0, C = prepare_network_data(sioux)\n",
    "\n",
    "directory = \"/home/podozerovapo/traffic_assignment/data/sioux/uncongested\"\n",
    "\n",
    "inputs = []\n",
    "outputs = []\n",
    "metadata = []\n",
    "\n",
    "for filename in sorted(os.listdir(directory)):\n",
    "    if filename.endswith(\".pkl\"):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        \n",
    "        with open(filepath, 'rb') as f:\n",
    "            data_pair = pickle.load(f)\n",
    "            \n",
    "            inputs.append(data_pair['input'])\n",
    "            outputs.append(data_pair['output'])\n",
    "            metadata.append(data_pair.get('metadata', None))\n",
    "\n",
    "input_matrices = np.array(inputs)  # [num_samples, num_nodes, num_nodes]\n",
    "output_matrices = np.array(outputs)  # [num_samples, num_nodes, num_nodes]\n",
    "\n",
    "print(f\"Loaded {len(inputs)} samples\")\n",
    "print(f\"Input shape: {input_matrices.shape}\")\n",
    "print(f\"Output shape: {output_matrices.shape}\")\n",
    "\n",
    "dataset = TrafficDataset(input_matrices, C, T_0, output_matrices)\n",
    "dataloader = dataset.get_batches(batch_size=128)\n",
    "num_nodes = len(dataset[0]['od_matrix'])\n",
    "num_edges = (dataset[0]['capacity'] > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5157112",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in dataloader:\n",
    "    a = i\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb1e2cab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 24, 24])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['od_matrix'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e035607a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e6065fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FlowDistributionModel(num_nodes, hidden_dim=32).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7ca8e90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (552x3 and 16x32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m             total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[104], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, epochs)\u001b[0m\n\u001b[1;32m     10\u001b[0m OD, C, T0, targets \u001b[38;5;241m=\u001b[39m OD\u001b[38;5;241m.\u001b[39mcuda(), C\u001b[38;5;241m.\u001b[39mcuda(), T0\u001b[38;5;241m.\u001b[39mcuda(), targets\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 13\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(preds, targets)\n\u001b[1;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/traffic_assignment/diplom_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/traffic_assignment/diplom_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[102], line 63\u001b[0m, in \u001b[0;36mFlowDistributionModel.forward\u001b[0;34m(self, OD, C, T0)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# GAT layers\u001b[39;00m\n\u001b[1;32m     62\u001b[0m h \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftplus(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgat1(x_graph, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_index))\n\u001b[0;32m---> 63\u001b[0m h \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftplus(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgat2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     64\u001b[0m h \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftplus(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgat3(h, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_index))\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Decode to flows\u001b[39;00m\n",
      "File \u001b[0;32m~/traffic_assignment/diplom_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/traffic_assignment/diplom_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/traffic_assignment/diplom_venv/lib/python3.10/site-packages/torch_geometric/nn/conv/gat_conv.py:302\u001b[0m, in \u001b[0;36mGATConv.forward\u001b[0;34m(self, x, edge_index, edge_attr, size, return_attention_weights)\u001b[0m\n\u001b[1;32m    299\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres(x)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m     x_src \u001b[38;5;241m=\u001b[39m x_dst \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, H, C)\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;66;03m# If the module is initialized as bipartite, transform source\u001b[39;00m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;66;03m# and destination node features separately:\u001b[39;00m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin_src \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin_dst \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/traffic_assignment/diplom_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/traffic_assignment/diplom_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/traffic_assignment/diplom_venv/lib/python3.10/site-packages/torch_geometric/nn/dense/linear.py:147\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    142\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m        x (torch.Tensor): The input features.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (552x3 and 16x32)"
     ]
    }
   ],
   "source": [
    "def train(model, dataloader, epochs=5):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            OD, C, T0, targets = batch.values()\n",
    "            # Move data to GPU\n",
    "            OD, C, T0, targets = OD.cuda(), C.cuda(), T0.cuda(), targets.cuda()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            preds = model(OD, C, T0)\n",
    "            loss = F.mse_loss(preds, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "train(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f6f204b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     10\u001b[0m pred_flows \u001b[38;5;241m=\u001b[39m model(OD_batch, C_batch, T0_batch)\n\u001b[0;32m---> 11\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_flows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_flows_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOD_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[57], line 3\u001b[0m, in \u001b[0;36mloss_fn\u001b[0;34m(pred_flows, target_flows, OD, C)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mloss_fn\u001b[39m(pred_flows, target_flows, OD, C):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Основная MSE потеря\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     mse_loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_flows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_flows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Потеря на сохранение потока (входной поток = выходной)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     inflow \u001b[38;5;241m=\u001b[39m pred_flows\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [batch, num_nodes]\u001b[39;00m\n",
      "File \u001b[0;32m~/traffic_assignment/diplom_venv/lib/python3.10/site-packages/torch/nn/functional.py:3905\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction, weight)\u001b[0m\n\u001b[1;32m   3901\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3902\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid reduction mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreduction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3903\u001b[0m         )\n\u001b[1;32m   3904\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3905\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3906\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpanded_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpanded_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3907\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "model = FlowDistributionModel(num_nodes=num_nodes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in tqdm(range(100)):\n",
    "    for batch in dataloader:\n",
    "        OD_batch, C_batch, T0_batch, target_flows_batch = batch.values()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred_flows = model(OD_batch, C_batch, T0_batch)\n",
    "        loss = loss_fn(pred_flows, target_flows_batch, OD_batch, C_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "94800ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['od_matrix', 'capacity', 'free_flow_time', 'link_flows'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f243f41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "OD, C, T_0, true = dataset[11].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d590ae40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 24])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OD.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a24595d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 894766.4375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3419362/4046493625.py:5: UserWarning: Using a target size (torch.Size([1, 24, 24])) that is different to the input size (torch.Size([24, 24])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(pred_flows, true.unsqueeze(0).cuda())\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pred_flows = model(OD.unsqueeze(0), C.unsqueeze(0), T_0.unsqueeze(0))[0]\n",
    "\n",
    "# Loss calculation\n",
    "pred_flows[true==0] = 0\n",
    "loss = F.mse_loss(pred_flows, true.unsqueeze(0).cuda())\n",
    "print(f\"Initial loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "effe8672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(380693.3750, device='cuda:0', grad_fn=<SumBackward0>), tensor(269510.))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(pred_flows), torch.sum(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f63cad88",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred_flows' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39mabs(\u001b[43mpred_flows\u001b[49m\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m true\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pred_flows' is not defined"
     ]
    }
   ],
   "source": [
    "torch.abs(pred_flows.sum(1) - true.sum(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "40f0d951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true[true==0].to(int).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2bd03970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(158915, device='cuda:0')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_flows[0][true==0].to(int).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "17a26098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0,    0, 3056,    0, 4744,    0,    0,    0,    0,    0, 2630,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true[3].to(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4a7c49e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  10,    8, 4703,    0, 4910,   21,    7,   22,   29,   34, 1390,   16,\n",
       "          17,   31,   33,   47,   23,    4,    5,    6,    8,    8,   25,    9],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_flows[0][3].to(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "160f95ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0, 1175, 2180,  165,  165,  165,  165,  165,  165,  165,  165,  165,\n",
       "          165,  165,  165,  165,  165,  165,  165,  165,  165,  165,  165,  165],\n",
       "        [ 988,    0,    2,    1,    3,  903,    1,    0,    1,    0,    1,    3,\n",
       "            0,    2,    2,    0,    2,    0,    2,    2,    0,    4,    0,    0],\n",
       "        [ 957,    2,    0, 1342,    2,    1,    2,    2,    2,    1,    0, 1162,\n",
       "            2,    2,    2,    2,    3,    0,    0,    0,    0,    2,    3,    0],\n",
       "        [   1,    1,  452,    0, 1305,    0,    1,    0,    1,    2,  509,    0,\n",
       "            0,    1,    1,    4,    0,    2,    2,    2,    1,    1,    0,    1],\n",
       "        [   1,    2,    2, 1756,    0, 3106,    1,    0,  322,    5,    1,    3,\n",
       "            2,    2,    1,    0,    2,    0,    2,    3,    2,    1,    4,    0],\n",
       "        [   0,  753,    1,    1, 2486,    0,    0, 3291,    0,    0,    0,    2,\n",
       "            1,    2,    2,    2,    0,    2,    2,    0,    3,    1,    2,    2],\n",
       "        [   1,    1,    2,    0,    1,    0,    0,  789,    2,   13,    1,    1,\n",
       "            1,    2,    0,    0,    0,  492,    1,    1,    2,    0,    1,    3],\n",
       "        [   0,    0,    1,    1,    1, 2759,  654,    0, 1776,    8,    4,    2,\n",
       "            0,    2,    0, 1279,    6,    1,    2,    4,    0,    0,    0,    2],\n",
       "        [   0,    1,    2,    2,  425,    0,    2, 1760,    0, 1288,    1,    1,\n",
       "            2,    0,    3,    0,    4,    1,    1,    0,    0,    1,    0,    1],\n",
       "        [   8,    1,    1,    1,    5,    2,    6,    6,  985,    0,  947,    5,\n",
       "            9,   14, 2153, 2570, 2970,    1,   11,   15,    6,   14,    7,    3],\n",
       "        [   1,    2,    0,  337,    0,    0,    1,    3,    6,   67,    0,  589,\n",
       "            5, 2042,    1,    6,    4,    2,    2,    2,    1,    0,    8,    1],\n",
       "        [   1,    2,  778,    0,    1,    1,    2,    1,    0,    2,  250,    0,\n",
       "         2832,    0,    1,    1,    0,    2,    0,    0,    1,    2,    1,    2],\n",
       "        [   0,    2,    4,    1,    1,    1,    1,    0,    0,    8,    0, 1766,\n",
       "            0,    1,    2,    1,    1,    2,    0,    0,    1,    9,    0, 2041],\n",
       "        [   0,    2,    2,    0,    3,    3,    1,    1,    1,    6, 1722,    1,\n",
       "            1,    0,  246,    2,    0,    3,    0,    1,    0,    3,  150,    0],\n",
       "        [   1,    3,    2,    1,    2,    1,    1,    0,    2, 2859,    7,    2,\n",
       "            2,  344,    0,    1,    9,    2, 2320,    0,    0, 1834,    0,    0],\n",
       "        [   0,    1,    1,    0,    1,    4,    6,  799,    2, 2890,    0,    1,\n",
       "            1,    1,    7,    0, 2396, 1286,    2,    8,    1,    4,    0,    2],\n",
       "        [   0,    1,    2,    0,    2,    0,    2,    9,    2, 3539,    0,    1,\n",
       "            1,    2,    9, 2955,    0,    1, 3356,   11,    1,   11,    2,    0],\n",
       "        [   3,    0,    0,    2,    0,    2,  190,    1,    2,    2,    1,    3,\n",
       "            2,    2,    1,  194,    1,    0,    1,  734,    2,    1,    2,    0],\n",
       "        [   1,    2,    0,    1,    2,    2,    0,    0,    0,    1,    0,    1,\n",
       "            2,    1, 3595,    2, 2278,    0,    0,  803,    0,    0,    1,    3],\n",
       "        [   0,    2,    0,    0,    3,    2,    0,    3,    0,   10,    1,    1,\n",
       "            0,    0,    5,   10,    8,   65,  436,    0,  902, 1244,    3,    0],\n",
       "        [   2,    0,    0,    2,    2,    3,    1,    1,    2,    1,    0,    0,\n",
       "            2,    0,    1,    0,    0,    2,    0, 1222,    0, 1352,    1, 2278],\n",
       "        [   0,    3,    3,    1,    2,    2,    0,    0,    1,   12,    3,    1,\n",
       "            5,    8, 1394,    2,    2,    2,    0, 1758, 1462,    0,   54,    2],\n",
       "        [   0,    0,    3,    0,    2,    2,    1,    1,    1,    6,    6,    1,\n",
       "            2,  834,    3,    0,    0,    2,    1,    1,    0,   50,    0,   49],\n",
       "        [   2,    0,    0,    1,    0,    2,    4,    1,    1,    2,    2,    0,\n",
       "         1280,    0,    0,    1,    1,    0,    3,    0, 3151,    4,  343,    0]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(true.cuda().to(int) - pred_flows[0].to(int))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diplom_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
