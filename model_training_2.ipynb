{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7eed1f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.nn import LayerNorm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "594f6838",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_coords = pd.read_csv(\"data/sioux/SiouxFalls_node.tntp\", sep='\\t')\n",
    "node_coords_arr = np.array(node_coords[['X', 'Y']])\n",
    "\n",
    "sioux = create_network_df(network_name=\"SiouxFalls\")\n",
    "T_0, C = prepare_network_data(sioux)\n",
    "\n",
    "directory = \"/home/podozerovapo/traffic_assignment/data/sioux/uncongested\"\n",
    "\n",
    "inputs = []\n",
    "outputs = []\n",
    "metadata = []\n",
    "\n",
    "for filename in sorted(os.listdir(directory)):\n",
    "    if filename.endswith(\".pkl\"):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        \n",
    "        with open(filepath, 'rb') as f:\n",
    "            data_pair = pickle.load(f)\n",
    "            \n",
    "            inputs.append(data_pair['input'])\n",
    "            outputs.append(data_pair['output'])\n",
    "            metadata.append(data_pair.get('metadata', None))\n",
    "\n",
    "input_matrices = np.array(inputs)  # [num_samples, num_nodes, num_nodes]\n",
    "output_matrices = np.array(outputs)  # [num_samples, num_nodes, num_nodes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17685714",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEmbedding(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size=32):\n",
    "        super(FeatureEmbedding, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, embedding_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class SharedEncoderComponents(nn.Module):\n",
    "    def __init__(self, node_feat_size, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.node_feat_size = node_feat_size\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = node_feat_size // n_heads\n",
    "        \n",
    "        self.q_linear = nn.Linear(node_feat_size, node_feat_size)\n",
    "        self.k_linear = nn.Linear(node_feat_size, node_feat_size)\n",
    "        self.v_linear = nn.Linear(node_feat_size, node_feat_size)\n",
    "        \n",
    "        self.out_ffn = nn.Sequential(\n",
    "            nn.Linear(node_feat_size, node_feat_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(node_feat_size, node_feat_size)\n",
    "        )\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(node_feat_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "class VEncoderLayer(nn.Module):\n",
    "    def __init__(self, node_feat_size, edge_feat_size, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.shared = SharedEncoderComponents(node_feat_size, n_heads, dropout)\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = node_feat_size // n_heads\n",
    "        self.edge_weight_ffn = nn.Sequential(\n",
    "            nn.Linear(2 * node_feat_size, edge_feat_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(edge_feat_size, n_heads),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(node_feat_size, node_feat_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(node_feat_size * 2, node_feat_size),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, adj_mask):\n",
    "        batch_size, num_nodes, _ = x.size()\n",
    "\n",
    "        q = self.shared.q_linear(x).view(batch_size, num_nodes, self.n_heads, self.head_dim)\n",
    "        k = self.shared.k_linear(x).view(batch_size, num_nodes, self.n_heads, self.head_dim)\n",
    "        v = self.shared.v_linear(x).view(batch_size, num_nodes, self.n_heads, self.head_dim)\n",
    "        \n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        src_nodes = x.unsqueeze(2).expand(-1, -1, num_nodes, -1)\n",
    "        dst_nodes = x.unsqueeze(1).expand(-1, num_nodes, -1, -1)\n",
    "        node_pairs = torch.cat([src_nodes, dst_nodes], dim=-1)\n",
    "        beta = self.edge_weight_ffn(node_pairs).permute(0, 3, 1, 2)\n",
    "        \n",
    "        adj_mask = adj_mask.unsqueeze(1)\n",
    "        scores = scores * adj_mask * beta\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.shared.dropout(attn_weights)\n",
    "        \n",
    "        output = torch.matmul(attn_weights, v)\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "        output = output.view(batch_size, num_nodes, self.shared.node_feat_size)\n",
    "        \n",
    "        output = self.shared.out_ffn(output)\n",
    "        output = self.shared.dropout(output)\n",
    "        output = self.shared.layer_norm(x + output)\n",
    "        \n",
    "        ffn_output = self.ffn(output)\n",
    "        output = self.shared.layer_norm(output + ffn_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class REncoderLayer(nn.Module):\n",
    "    def __init__(self, node_feat_size, edge_feat_size, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.shared = SharedEncoderComponents(node_feat_size, n_heads, dropout)\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = node_feat_size // n_heads\n",
    "        self.edge_feat_transform = nn.Linear(edge_feat_size, n_heads)\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(node_feat_size, node_feat_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(node_feat_size * 2, node_feat_size),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, adj_mask, edge_features):\n",
    "        batch_size, num_nodes, _ = x.size()\n",
    "        \n",
    "        q = self.shared.q_linear(x).view(batch_size, num_nodes, self.n_heads, self.head_dim)\n",
    "        k = self.shared.k_linear(x).view(batch_size, num_nodes, self.n_heads, self.head_dim)\n",
    "        v = self.shared.v_linear(x).view(batch_size, num_nodes, self.n_heads, self.head_dim)\n",
    "        \n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        edge_weights = self.edge_feat_transform(edge_features).permute(0, 3, 1, 2)\n",
    "    \n",
    "        adj_mask = adj_mask.unsqueeze(1)\n",
    "        scores = scores * adj_mask * torch.sigmoid(edge_weights)\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.shared.dropout(attn_weights)\n",
    "        \n",
    "        output = torch.matmul(attn_weights, v)\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "        output = output.view(batch_size, num_nodes, self.shared.node_feat_size)\n",
    "        \n",
    "        output = self.shared.out_ffn(output)\n",
    "        output = self.shared.dropout(output)\n",
    "        output = self.shared.layer_norm(x + output)\n",
    "        \n",
    "        ffn_output = self.ffn(output)\n",
    "        output = self.shared.layer_norm(output + ffn_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class VEncoder(nn.Module):\n",
    "    def __init__(self, node_feat_size, edge_feat_size, n_layers=3, n_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            VEncoderLayer(node_feat_size, edge_feat_size, n_heads, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x, v_adj_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, v_adj_mask)\n",
    "        return x\n",
    "\n",
    "class REncoder(nn.Module):\n",
    "    def __init__(self, node_feat_size, edge_feat_size, n_layers=3, n_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            REncoderLayer(node_feat_size, edge_feat_size, n_heads, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x, adj_mask, edge_features):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, adj_mask, edge_features)\n",
    "        return x\n",
    "\n",
    "class DualGraphEncoder(nn.Module):\n",
    "    def __init__(self, node_feat_size, edge_feat_size,\n",
    "                 v_layers=3, r_layers=3, n_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.vencoder = VEncoder(node_feat_size, edge_feat_size, v_layers, n_heads, dropout)\n",
    "        self.rencoder = REncoder(node_feat_size, edge_feat_size, r_layers, n_heads, dropout)\n",
    "\n",
    "    def forward(self, node_features, real_adj_mask, virtual_adj_mask, edge_features):\n",
    "        v_encoded = self.vencoder(node_features, virtual_adj_mask)\n",
    "        r_encoded = self.rencoder(node_features, real_adj_mask, edge_features)\n",
    "        return v_encoded, r_encoded\n",
    "\n",
    "class TrafficAssignmentModel(nn.Module):\n",
    "    def __init__(self, num_nodes, node_feat_size=32, edge_feat_size=2,\n",
    "                 v_layers=2, r_layers=2, n_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.feature_preprocessor = FeatureEmbedding(input_size=num_nodes + 2, embedding_size=32)\n",
    "        \n",
    "        self.dual_encoder = DualGraphEncoder(node_feat_size, edge_feat_size, \n",
    "                                           v_layers, r_layers, n_heads, dropout)\n",
    "        \n",
    "        self.flow_predictor = nn.Sequential(\n",
    "            nn.Linear(4 * node_feat_size, node_feat_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(node_feat_size, 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, node_features, real_adj_mask, virt_adj_mask, edge_features):\n",
    "        \n",
    "        node_features = self.feature_preprocessor(node_features)\n",
    "        v_encoded, r_encoded = self.dual_encoder(node_features, real_adj_mask, virt_adj_mask, edge_features)\n",
    "        combined = torch.cat([v_encoded, r_encoded], dim=-1)\n",
    "        src_nodes = combined.unsqueeze(2).expand(-1, -1, combined.size(1), -1)\n",
    "        dst_nodes = combined.unsqueeze(1).expand(-1, combined.size(1), -1, -1)\n",
    "        pair_features = torch.cat([src_nodes, dst_nodes], dim=-1)\n",
    "\n",
    "        flows = self.flow_predictor(pair_features).squeeze(-1)\n",
    "        flows = flows * real_adj_mask\n",
    "        \n",
    "        return flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fede244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_for_concat = np.repeat(np.expand_dims(node_coords_arr, 0), repeats=input_matrices.shape[0],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6efc6ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4096, 24, 26)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.concatenate([input_matrices, coords_for_concat], axis=2)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0875fd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "capacities = np.repeat(np.expand_dims(C, 0), axis=0, repeats=input_matrices.shape[0])\n",
    "free_flow_times = np.repeat(np.expand_dims(T_0, 0), axis=0, repeats=input_matrices.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed88027",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficDataset(Dataset):\n",
    "    def __init__(self, X_data, capacity_data, free_flow_data, flows_data):\n",
    "        self.data = []\n",
    "        for x, cap, fft, fl in zip(X_data, capacity_data, free_flow_data, flows_data):\n",
    "            od_matrix = x[:, :-2]\n",
    "            coordinates = x[:, -2:]\n",
    "\n",
    "            od_mean = od_matrix.mean()\n",
    "            od_std = od_matrix.std() + 1e-8\n",
    "            od_matrix = (od_matrix - od_mean) / od_std\n",
    "\n",
    "            coord_mean = coordinates.mean(axis=0)\n",
    "            coord_std = coordinates.std(axis=0) + 1e-8\n",
    "            coordinates = (coordinates - coord_mean) / coord_std\n",
    "\n",
    "            x_normalized = np.concatenate([od_matrix, coordinates], axis=1)\n",
    "\n",
    "            cap = (cap - cap.mean()) / (cap.std() + 1e-8)\n",
    "            fft = (fft - fft.mean()) / (fft.std() + 1e-8)\n",
    "            fl = (fl - fl.mean()) / (fl.std() + 1e-8)\n",
    "\n",
    "            self.data.append({\n",
    "                'od_matrix': torch.FloatTensor(od_matrix),\n",
    "                'coordinates': torch.FloatTensor(coordinates),\n",
    "                'capacity': torch.FloatTensor(cap),\n",
    "                'free_flow': torch.FloatTensor(fft),\n",
    "                'flows': torch.FloatTensor(fl),\n",
    "                'x_normalized' : torch.FloatTensor(x_normalized)\n",
    "            })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        real_adj_mask = (item['capacity'] > 0).float()\n",
    "        virtual_adj = item['od_matrix'].abs()\n",
    "\n",
    "        edge_features = torch.stack([\n",
    "            item['capacity'],\n",
    "            item['free_flow']\n",
    "        ], dim=-1)\n",
    "\n",
    "        agg_demand = item['od_matrix'].sum(dim=1, keepdim=True)\n",
    "        node_features = torch.cat([item['coordinates'], agg_demand], dim=1)\n",
    "\n",
    "        return {\n",
    "            'node_features': node_features,\n",
    "            'real_adj_mask': real_adj_mask,\n",
    "            'edge_features': edge_features,\n",
    "            'virtual_adj': virtual_adj,\n",
    "            'target_flows': item['flows'],\n",
    "            'x_normalized' : item['x_normalized']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaf3d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=100, lr=1e-3, patience=5):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=3)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            node_feat = batch['x_normalized'].to(device)\n",
    "            real_mask = batch['real_adj_mask'].to(device)\n",
    "            edge_feat = batch['edge_features'].to(device)\n",
    "            virt_adj = batch['virtual_adj'].to(device)\n",
    "            targets = batch['target_flows'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(node_feat, real_mask, virt_adj, edge_feat)\n",
    "            loss = criterion(outputs[real_mask.bool()], targets[real_mask.bool()])\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * node_feat.size(0)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                node_feat = batch['x_normalized'].to(device)\n",
    "                real_mask = batch['real_adj_mask'].to(device)\n",
    "                edge_feat = batch['edge_features'].to(device)\n",
    "                virt_adj = batch['virtual_adj'].to(device)\n",
    "                targets = batch['target_flows'].to(device)\n",
    "                \n",
    "                outputs = model(node_feat, real_mask, virt_adj, edge_feat)\n",
    "                \n",
    "                loss = criterion(outputs[real_mask.bool()], targets[real_mask.bool()])\n",
    "                val_loss += loss.item() * node_feat.size(0)\n",
    "        \n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        train_history.append(train_loss)\n",
    "        val_history.append(val_loss)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        print(f'Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')\n",
    "        print(f'LR: {optimizer.param_groups[0][\"lr\"]:.2e}')\n",
    "        \n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == patience:\n",
    "                print(f'Early stopping after {epoch+1} epochs')\n",
    "                break\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_history, label='Train Loss')\n",
    "    plt.plot(val_history, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training History')\n",
    "    plt.legend()\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.close()\n",
    "    \n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a06c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(model, data_loader, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc='Making predictions'):\n",
    "            node_feat = batch['x_normalized'].to(device)\n",
    "            real_mask = batch['real_adj_mask'].to(device)\n",
    "            edge_feat = batch['edge_features'].to(device)\n",
    "            virt_adj = batch['virtual_adj'].to(device)\n",
    "            targets = batch['target_flows'].to(device)\n",
    "\n",
    "            outputs = model(node_feat, real_mask, virt_adj, edge_feat)\n",
    "\n",
    "            mask = real_mask.bool()\n",
    "            all_predictions.append(outputs[mask].cpu().numpy())\n",
    "            all_targets.append(targets[mask].cpu().numpy())\n",
    "            break\n",
    "\n",
    "    predictions = np.concatenate(all_predictions)\n",
    "    targets = np.concatenate(all_targets)\n",
    "\n",
    "    return predictions, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6a055b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making predictions:   0%|          | 0/52 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "ans = model_predict(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d2e2e274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.48642147 0.9556702  0.5360018  ... 2.403835   2.7833183  1.0279554 ]\n",
      "[0.4724989  0.96577114 0.63058925 ... 2.6740074  2.8549805  0.9699092 ]\n"
     ]
    }
   ],
   "source": [
    "print(ans[0], ans[1], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:03<05:46,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Train Loss: 1.7905 | Val Loss: 0.8613\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:06<05:42,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "Train Loss: 0.6359 | Val Loss: 0.3121\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [00:10<05:47,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100\n",
      "Train Loss: 0.1756 | Val Loss: 0.0997\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [00:14<05:37,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n",
      "Train Loss: 0.0990 | Val Loss: 0.0806\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [00:17<05:43,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "Train Loss: 0.0864 | Val Loss: 0.0763\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [00:21<05:30,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "Train Loss: 0.0797 | Val Loss: 0.0718\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [00:24<05:26,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100\n",
      "Train Loss: 0.0746 | Val Loss: 0.0678\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [00:28<05:17,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100\n",
      "Train Loss: 0.0713 | Val Loss: 0.0669\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [00:31<05:20,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100\n",
      "Train Loss: 0.0688 | Val Loss: 0.0634\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [00:35<05:19,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100\n",
      "Train Loss: 0.0664 | Val Loss: 0.0617\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [00:39<05:19,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100\n",
      "Train Loss: 0.0646 | Val Loss: 0.0603\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [00:42<05:16,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100\n",
      "Train Loss: 0.0628 | Val Loss: 0.0582\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [00:45<05:06,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100\n",
      "Train Loss: 0.0608 | Val Loss: 0.0587\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [00:49<05:02,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100\n",
      "Train Loss: 0.0590 | Val Loss: 0.0551\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [00:53<05:00,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100\n",
      "Train Loss: 0.0580 | Val Loss: 0.0545\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [00:56<04:47,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100\n",
      "Train Loss: 0.0561 | Val Loss: 0.0521\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/100 [00:59<04:36,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100\n",
      "Train Loss: 0.0546 | Val Loss: 0.0526\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18/100 [01:02<04:29,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100\n",
      "Train Loss: 0.0537 | Val Loss: 0.0502\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19/100 [01:04<04:03,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100\n",
      "Train Loss: 0.0519 | Val Loss: 0.0496\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [01:07<03:50,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100\n",
      "Train Loss: 0.0510 | Val Loss: 0.0504\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [01:10<04:01,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100\n",
      "Train Loss: 0.0507 | Val Loss: 0.0478\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22/100 [01:14<04:07,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100\n",
      "Train Loss: 0.0500 | Val Loss: 0.0481\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 23/100 [01:17<04:05,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100\n",
      "Train Loss: 0.0482 | Val Loss: 0.0468\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24/100 [01:21<04:09,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100\n",
      "Train Loss: 0.0475 | Val Loss: 0.0462\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25/100 [01:24<04:09,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100\n",
      "Train Loss: 0.0468 | Val Loss: 0.0456\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 26/100 [01:28<04:11,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100\n",
      "Train Loss: 0.0468 | Val Loss: 0.0438\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 27/100 [01:31<04:09,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100\n",
      "Train Loss: 0.0460 | Val Loss: 0.0451\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 28/100 [01:35<04:10,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100\n",
      "Train Loss: 0.0453 | Val Loss: 0.0444\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 29/100 [01:38<04:08,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100\n",
      "Train Loss: 0.0449 | Val Loss: 0.0425\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [01:42<04:07,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100\n",
      "Train Loss: 0.0443 | Val Loss: 0.0430\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [01:45<04:03,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100\n",
      "Train Loss: 0.0440 | Val Loss: 0.0420\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32/100 [01:49<04:04,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100\n",
      "Train Loss: 0.0437 | Val Loss: 0.0427\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 33/100 [01:53<04:01,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100\n",
      "Train Loss: 0.0434 | Val Loss: 0.0413\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 34/100 [01:56<03:52,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100\n",
      "Train Loss: 0.0422 | Val Loss: 0.0426\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35/100 [02:00<03:47,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/100\n",
      "Train Loss: 0.0419 | Val Loss: 0.0397\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 36/100 [02:03<03:43,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/100\n",
      "Train Loss: 0.0415 | Val Loss: 0.0405\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 37/100 [02:06<03:39,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/100\n",
      "Train Loss: 0.0409 | Val Loss: 0.0390\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 38/100 [02:10<03:37,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/100\n",
      "Train Loss: 0.0404 | Val Loss: 0.0405\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 39/100 [02:14<03:33,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/100\n",
      "Train Loss: 0.0397 | Val Loss: 0.0395\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [02:17<03:31,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/100\n",
      "Train Loss: 0.0396 | Val Loss: 0.0379\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 41/100 [02:20<03:24,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/100\n",
      "Train Loss: 0.0391 | Val Loss: 0.0402\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42/100 [02:24<03:19,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/100\n",
      "Train Loss: 0.0389 | Val Loss: 0.0383\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 43/100 [02:27<03:15,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/100\n",
      "Train Loss: 0.0382 | Val Loss: 0.0362\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44/100 [02:30<03:08,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/100\n",
      "Train Loss: 0.0376 | Val Loss: 0.0365\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 45/100 [02:34<03:07,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100\n",
      "Train Loss: 0.0376 | Val Loss: 0.0359\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 46/100 [02:37<03:06,  3.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/100\n",
      "Train Loss: 0.0368 | Val Loss: 0.0377\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 47/100 [02:41<03:05,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/100\n",
      "Train Loss: 0.0367 | Val Loss: 0.0353\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 48/100 [02:45<03:01,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/100\n",
      "Train Loss: 0.0364 | Val Loss: 0.0361\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 49/100 [02:48<02:59,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100\n",
      "Train Loss: 0.0362 | Val Loss: 0.0353\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [02:52<02:55,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100\n",
      "Train Loss: 0.0362 | Val Loss: 0.0345\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 51/100 [02:55<02:51,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100\n",
      "Train Loss: 0.0360 | Val Loss: 0.0367\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 52/100 [02:59<02:48,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/100\n",
      "Train Loss: 0.0360 | Val Loss: 0.0340\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 53/100 [03:02<02:43,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/100\n",
      "Train Loss: 0.0350 | Val Loss: 0.0337\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 54/100 [03:06<02:41,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/100\n",
      "Train Loss: 0.0350 | Val Loss: 0.0337\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 55/100 [03:08<02:27,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100\n",
      "Train Loss: 0.0347 | Val Loss: 0.0348\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 56/100 [03:12<02:24,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100\n",
      "Train Loss: 0.0343 | Val Loss: 0.0330\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 57/100 [03:15<02:24,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "Train Loss: 0.0343 | Val Loss: 0.0331\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 58/100 [03:19<02:22,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "Train Loss: 0.0339 | Val Loss: 0.0327\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 59/100 [03:22<02:19,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "Train Loss: 0.0335 | Val Loss: 0.0326\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [03:26<02:16,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/100\n",
      "Train Loss: 0.0335 | Val Loss: 0.0326\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 61/100 [03:29<02:14,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100\n",
      "Train Loss: 0.0332 | Val Loss: 0.0322\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 62/100 [03:33<02:12,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100\n",
      "Train Loss: 0.0333 | Val Loss: 0.0319\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 63/100 [03:36<02:06,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/100\n",
      "Train Loss: 0.0328 | Val Loss: 0.0339\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 64/100 [03:39<01:56,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/100\n",
      "Train Loss: 0.0332 | Val Loss: 0.0316\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 65/100 [03:42<01:56,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/100\n",
      "Train Loss: 0.0325 | Val Loss: 0.0317\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 66/100 [03:45<01:42,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/100\n",
      "Train Loss: 0.0321 | Val Loss: 0.0307\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 67/100 [03:48<01:43,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/100\n",
      "Train Loss: 0.0321 | Val Loss: 0.0323\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 68/100 [03:51<01:43,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/100\n",
      "Train Loss: 0.0321 | Val Loss: 0.0316\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 69/100 [03:55<01:43,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/100\n",
      "Train Loss: 0.0317 | Val Loss: 0.0309\n",
      "LR: 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70/100 [03:59<01:41,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/100\n",
      "Train Loss: 0.0317 | Val Loss: 0.0323\n",
      "LR: 5.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 71/100 [04:02<01:38,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/100\n",
      "Train Loss: 0.0309 | Val Loss: 0.0298\n",
      "LR: 5.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 72/100 [04:06<01:36,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/100\n",
      "Train Loss: 0.0305 | Val Loss: 0.0306\n",
      "LR: 5.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 73/100 [04:09<01:33,  3.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/100\n",
      "Train Loss: 0.0305 | Val Loss: 0.0291\n",
      "LR: 5.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 74/100 [04:13<01:30,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/100\n",
      "Train Loss: 0.0302 | Val Loss: 0.0291\n",
      "LR: 5.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 75/100 [04:16<01:27,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/100\n",
      "Train Loss: 0.0301 | Val Loss: 0.0295\n",
      "LR: 5.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 76/100 [04:20<01:23,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/100\n",
      "Train Loss: 0.0302 | Val Loss: 0.0291\n",
      "LR: 5.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 77/100 [04:23<01:20,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100\n",
      "Train Loss: 0.0299 | Val Loss: 0.0295\n",
      "LR: 2.50e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 78/100 [04:27<01:17,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100\n",
      "Train Loss: 0.0296 | Val Loss: 0.0288\n",
      "LR: 2.50e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 79/100 [04:30<01:13,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/100\n",
      "Train Loss: 0.0295 | Val Loss: 0.0284\n",
      "LR: 2.50e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80/100 [04:33<01:09,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "Train Loss: 0.0294 | Val Loss: 0.0284\n",
      "LR: 2.50e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 81/100 [04:37<01:05,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/100\n",
      "Train Loss: 0.0294 | Val Loss: 0.0285\n",
      "LR: 2.50e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 82/100 [04:40<01:02,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100\n",
      "Train Loss: 0.0293 | Val Loss: 0.0284\n",
      "LR: 2.50e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 83/100 [04:44<00:58,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/100\n",
      "Train Loss: 0.0293 | Val Loss: 0.0285\n",
      "LR: 2.50e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 84/100 [04:47<00:55,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/100\n",
      "Train Loss: 0.0292 | Val Loss: 0.0282\n",
      "LR: 2.50e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 85/100 [04:51<00:52,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/100\n",
      "Train Loss: 0.0292 | Val Loss: 0.0282\n",
      "LR: 2.50e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 86/100 [04:54<00:48,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/100\n",
      "Train Loss: 0.0292 | Val Loss: 0.0281\n",
      "LR: 2.50e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 87/100 [04:58<00:45,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/100\n",
      "Train Loss: 0.0291 | Val Loss: 0.0283\n",
      "LR: 2.50e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 88/100 [05:01<00:41,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/100\n",
      "Train Loss: 0.0291 | Val Loss: 0.0279\n",
      "LR: 2.50e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 89/100 [05:05<00:37,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/100\n",
      "Train Loss: 0.0290 | Val Loss: 0.0282\n",
      "LR: 2.50e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90/100 [05:08<00:34,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/100\n",
      "Train Loss: 0.0290 | Val Loss: 0.0281\n",
      "LR: 2.50e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 91/100 [05:12<00:30,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/100\n",
      "Train Loss: 0.0289 | Val Loss: 0.0280\n",
      "LR: 2.50e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 92/100 [05:15<00:27,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/100\n",
      "Train Loss: 0.0289 | Val Loss: 0.0281\n",
      "LR: 1.25e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 93/100 [05:18<00:23,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/100\n",
      "Train Loss: 0.0287 | Val Loss: 0.0277\n",
      "LR: 1.25e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 94/100 [05:22<00:20,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/100\n",
      "Train Loss: 0.0286 | Val Loss: 0.0276\n",
      "LR: 1.25e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 95/100 [05:25<00:16,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/100\n",
      "Train Loss: 0.0286 | Val Loss: 0.0275\n",
      "LR: 1.25e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 96/100 [05:28<00:13,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/100\n",
      "Train Loss: 0.0286 | Val Loss: 0.0275\n",
      "LR: 1.25e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 97/100 [05:32<00:10,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/100\n",
      "Train Loss: 0.0285 | Val Loss: 0.0276\n",
      "LR: 1.25e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 98/100 [05:35<00:06,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/100\n",
      "Train Loss: 0.0286 | Val Loss: 0.0275\n",
      "LR: 1.25e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 99/100 [05:39<00:03,  3.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/100\n",
      "Train Loss: 0.0285 | Val Loss: 0.0275\n",
      "LR: 6.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [05:43<00:00,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100\n",
      "Train Loss: 0.0284 | Val Loss: 0.0273\n",
      "LR: 6.25e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_nodes = 24\n",
    "X_data = X\n",
    "capacity_data = capacities\n",
    "free_flow_data = free_flow_times\n",
    "flows_data = output_matrices\n",
    "\n",
    "full_dataset = TrafficDataset(X_data, capacity_data, free_flow_data, flows_data)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "model = TrafficAssignmentModel(24)\n",
    "\n",
    "# Запуск обучения\n",
    "trained_model = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs=100,\n",
    "    lr=1e-3,\n",
    "    patience=10\n",
    ")\n",
    "\n",
    "# Сохранение финальной модели\n",
    "torch.save(trained_model.state_dict(), 'final_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e035607a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e6065fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FlowDistributionModel(num_nodes, hidden_dim=32).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7ca8e90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (552x3 and 16x32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m             total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[104], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, epochs)\u001b[0m\n\u001b[1;32m     10\u001b[0m OD, C, T0, targets \u001b[38;5;241m=\u001b[39m OD\u001b[38;5;241m.\u001b[39mcuda(), C\u001b[38;5;241m.\u001b[39mcuda(), T0\u001b[38;5;241m.\u001b[39mcuda(), targets\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 13\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(preds, targets)\n\u001b[1;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/traffic_assignment/diplom_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/traffic_assignment/diplom_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[102], line 63\u001b[0m, in \u001b[0;36mFlowDistributionModel.forward\u001b[0;34m(self, OD, C, T0)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# GAT layers\u001b[39;00m\n\u001b[1;32m     62\u001b[0m h \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftplus(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgat1(x_graph, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_index))\n\u001b[0;32m---> 63\u001b[0m h \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftplus(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgat2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     64\u001b[0m h \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftplus(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgat3(h, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_index))\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Decode to flows\u001b[39;00m\n",
      "File \u001b[0;32m~/traffic_assignment/diplom_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/traffic_assignment/diplom_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/traffic_assignment/diplom_venv/lib/python3.10/site-packages/torch_geometric/nn/conv/gat_conv.py:302\u001b[0m, in \u001b[0;36mGATConv.forward\u001b[0;34m(self, x, edge_index, edge_attr, size, return_attention_weights)\u001b[0m\n\u001b[1;32m    299\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres(x)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m     x_src \u001b[38;5;241m=\u001b[39m x_dst \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, H, C)\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;66;03m# If the module is initialized as bipartite, transform source\u001b[39;00m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;66;03m# and destination node features separately:\u001b[39;00m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin_src \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin_dst \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/traffic_assignment/diplom_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/traffic_assignment/diplom_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/traffic_assignment/diplom_venv/lib/python3.10/site-packages/torch_geometric/nn/dense/linear.py:147\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    142\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m        x (torch.Tensor): The input features.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (552x3 and 16x32)"
     ]
    }
   ],
   "source": [
    "def train(model, dataloader, epochs=5):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            OD, C, T0, targets = batch.values()\n",
    "            # Move data to GPU\n",
    "            OD, C, T0, targets = OD.cuda(), C.cuda(), T0.cuda(), targets.cuda()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            preds = model(OD, C, T0)\n",
    "            loss = F.mse_loss(preds, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "train(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f6f204b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     10\u001b[0m pred_flows \u001b[38;5;241m=\u001b[39m model(OD_batch, C_batch, T0_batch)\n\u001b[0;32m---> 11\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_flows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_flows_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOD_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[57], line 3\u001b[0m, in \u001b[0;36mloss_fn\u001b[0;34m(pred_flows, target_flows, OD, C)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mloss_fn\u001b[39m(pred_flows, target_flows, OD, C):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Основная MSE потеря\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     mse_loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_flows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_flows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Потеря на сохранение потока (входной поток = выходной)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     inflow \u001b[38;5;241m=\u001b[39m pred_flows\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [batch, num_nodes]\u001b[39;00m\n",
      "File \u001b[0;32m~/traffic_assignment/diplom_venv/lib/python3.10/site-packages/torch/nn/functional.py:3905\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction, weight)\u001b[0m\n\u001b[1;32m   3901\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3902\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid reduction mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreduction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3903\u001b[0m         )\n\u001b[1;32m   3904\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3905\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3906\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpanded_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpanded_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3907\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "model = FlowDistributionModel(num_nodes=num_nodes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in tqdm(range(100)):\n",
    "    for batch in dataloader:\n",
    "        OD_batch, C_batch, T0_batch, target_flows_batch = batch.values()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred_flows = model(OD_batch, C_batch, T0_batch)\n",
    "        loss = loss_fn(pred_flows, target_flows_batch, OD_batch, C_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diplom_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
