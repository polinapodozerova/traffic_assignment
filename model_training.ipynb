{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "383bfeed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pykan\n",
      "  Downloading pykan-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
      "Downloading pykan-0.2.8-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: pykan\n",
      "Successfully installed pykan-0.2.8\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pykan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16afa157",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from utils import *\n",
    "\n",
    "node_coords = pd.read_csv(\"data/sioux/SiouxFalls_node.tntp\", sep='\\t')\n",
    "\n",
    "sioux = create_network_df(network_name=\"SiouxFalls\")\n",
    "T_0, C = prepare_network_data(sioux)\n",
    "\n",
    "directory = \"/home/podozerovapo/traffic_assignment/data/sioux/uncongested\"\n",
    "\n",
    "inputs = []\n",
    "outputs = []\n",
    "metadata = []\n",
    "\n",
    "for filename in sorted(os.listdir(directory)):\n",
    "    if filename.endswith(\".pkl\"):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        \n",
    "        with open(filepath, 'rb') as f:\n",
    "            data_pair = pickle.load(f)\n",
    "            \n",
    "            inputs.append(data_pair['input'])\n",
    "            outputs.append(data_pair['output'])\n",
    "            metadata.append(data_pair.get('metadata', None))\n",
    "\n",
    "input_matrices = np.array(inputs)  # [num_samples, num_nodes, num_nodes]\n",
    "output_matrices = np.array(outputs)  # [num_samples, num_nodes, num_nodes]\n",
    "\n",
    "\n",
    "class FeatureEmbedding(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size=32):\n",
    "        super(FeatureEmbedding, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, embedding_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "node_coords_arr = np.array(node_coords[['X', 'Y']])\n",
    "node_coords_arr.shape\n",
    "\n",
    "np.expand_dims(node_coords_arr, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d47a228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2176a6ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "838c5c51",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mF\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdcf536",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TrafficGCNN(nn.Module):\n",
    "    def __init__(self, num_nodes, num_edges, device='cpu'):\n",
    "        \"\"\"\n",
    "        GCNN for Traffic Assignment Problem\n",
    "        \n",
    "        Args:\n",
    "            num_nodes (int): Number of nodes in the network\n",
    "            num_edges (int): Number of edges in the network\n",
    "            device (str): Computation device ('cpu' or 'cuda')\n",
    "        \"\"\"\n",
    "        super(TrafficGCNN, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_edges = num_edges\n",
    "        self.device = device\n",
    "        \n",
    "        # Layer 1: Graph Convolution Layer\n",
    "        self.theta = nn.Parameter(torch.randn(num_nodes, num_nodes))\n",
    "        \n",
    "        # Layer 2: Flow Distribution Layer\n",
    "        self.W_q = nn.Parameter(torch.randn(num_nodes, num_edges))\n",
    "        \n",
    "        # Output Layer: Flow Aggregation Layer\n",
    "        self.W_F = nn.Parameter(torch.randn(num_nodes))\n",
    "        \n",
    "    def forward(self, X, A_w, D_w_bar):\n",
    "        \"\"\"\n",
    "        Forward pass of the GCNN\n",
    "        \n",
    "        Args:\n",
    "            X (torch.Tensor): OD demand matrix [batch_size, num_nodes, num_nodes]\n",
    "            A_w (torch.Tensor): Weighted adjacency matrix [num_nodes, num_nodes]\n",
    "            D_w_bar (torch.Tensor): Weighted degree matrix with self-loops [num_nodes, num_nodes]\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Predicted link flows [batch_size, num_edges]\n",
    "        \"\"\"\n",
    "        # Layer 1: Graph Convolution with diffusion process\n",
    "        # Compute normalized adjacency with self-loops\n",
    "        A_w_bar = A_w + torch.eye(self.num_nodes).to(self.device)\n",
    "        transition_matrix = torch.inverse(D_w_bar) @ A_w_bar\n",
    "        \n",
    "        # Diffusion convolution operation (Equation 11)\n",
    "        H1 = torch.tanh(self.theta @ transition_matrix @ X)\n",
    "        \n",
    "        # Layer 2: Flow distribution to links\n",
    "        H2 = torch.tanh(H1 @ self.W_q)\n",
    "        \n",
    "        # Output Layer: Flow aggregation\n",
    "        F = (H2.transpose(1, 2) @ self.W_F)\n",
    "        \n",
    "        return F\n",
    "    \n",
    "    def laplacian_forward(self, X, A_w, D_w_bar):\n",
    "        \"\"\"\n",
    "        Alternative forward pass using Laplacian matrix (Equation 14)\n",
    "        \"\"\"\n",
    "        # Compute normalized Laplacian\n",
    "        A_w_bar = A_w + torch.eye(self.num_nodes).to(self.device)\n",
    "        L_w_bar = D_w_bar - A_w_bar\n",
    "        norm_lap = torch.inverse(D_w_bar) @ L_w_bar\n",
    "        \n",
    "        # Laplacian-based convolution\n",
    "        H1 = torch.tanh(self.theta @ (torch.eye(self.num_nodes).to(self.device) - norm_lap) @ X)\n",
    "        \n",
    "        # Rest of the network remains the same\n",
    "        H2 = torch.tanh(H1 @ self.W_q)\n",
    "        F = (H2.transpose(1, 2) @ self.W_F)\n",
    "        \n",
    "        return F\n",
    "\n",
    "\n",
    "class TrafficAssignmentModel:\n",
    "    def __init__(self, num_nodes, num_edges, device='cpu'):\n",
    "        self.gcnn = TrafficGCNN(num_nodes, num_edges, device)\n",
    "        self.device = device\n",
    "        self.gcnn.to(device)\n",
    "        \n",
    "    def train(self, dataset, epochs=100, lr=0.001, batch_size=32):\n",
    "        \"\"\"\n",
    "        Train the GCNN model\n",
    "        \n",
    "        Args:\n",
    "            dataset: TrafficDataset object containing:\n",
    "                - OD matrices\n",
    "                - Capacity matrices (C)\n",
    "                - Free-flow time matrices (T0)\n",
    "                - Ground truth link flows\n",
    "            epochs (int): Number of training epochs\n",
    "            lr (float): Learning rate\n",
    "            batch_size (int): Batch size for training\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.Adam(self.gcnn.parameters(), lr=lr)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for batch in dataset.get_batches(batch_size):\n",
    "                # Prepare batch data\n",
    "                X_batch = batch['od_matrix'].to(self.device)\n",
    "                C_batch = batch['capacity'].to(self.device)\n",
    "                T0_batch = batch['free_flow_time'].to(self.device)\n",
    "                y_batch = batch['link_flows'].to(self.device)\n",
    "                \n",
    "                # Create weighted adjacency matrix (A_w)\n",
    "                # Using inverse of free-flow time as weights\n",
    "                A_w = self._create_weighted_adjacency(T0_batch)\n",
    "                \n",
    "                # Create degree matrix (D_w_bar)\n",
    "                D_w_bar = self._create_degree_matrix(A_w)\n",
    "                \n",
    "                # Forward pass\n",
    "                pred_flows = self.gcnn(X_batch, A_w, D_w_bar)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = criterion(pred_flows, y_batch)\n",
    "                \n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataset):.4f}\")\n",
    "    \n",
    "    def _create_weighted_adjacency(self, T0):\n",
    "        \"\"\"\n",
    "        Create weighted adjacency matrix from free-flow time\n",
    "        Weights are inverse of free-flow time (Equation 1 in paper)\n",
    "        \"\"\"\n",
    "        # Add small constant to avoid division by zero\n",
    "        weights = 1 / (T0 + 1e-6)\n",
    "        # Zero out diagonal (no self-loops here, they'll be added later)\n",
    "        mask = torch.eye(T0.size(1)).to(self.device)\n",
    "        return weights * (1 - mask)\n",
    "    \n",
    "    def _create_degree_matrix(self, A_w):\n",
    "        \"\"\"\n",
    "        Create degree matrix D_w_bar from weighted adjacency matrix\n",
    "        with self-loops (Equation 7)\n",
    "        \"\"\"\n",
    "        A_w_bar = A_w + torch.eye(A_w.size(1)).to(self.device)\n",
    "        return torch.diag(A_w_bar.sum(dim=1))\n",
    "    \n",
    "    def predict(self, X, C, T0):\n",
    "        \"\"\"\n",
    "        Predict link flows for given OD matrix, capacity and free-flow time\n",
    "        \"\"\"\n",
    "        self.gcnn.eval()\n",
    "        with torch.no_grad():\n",
    "            X = X.to(self.device)\n",
    "            C = C.to(self.device)\n",
    "            T0 = T0.to(self.device)\n",
    "            \n",
    "            A_w = self._create_weighted_adjacency(T0)\n",
    "            D_w_bar = self._create_degree_matrix(A_w)\n",
    "            \n",
    "            return self.gcnn(X, A_w, D_w_bar)\n",
    "\n",
    "\n",
    "class TrafficDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for traffic assignment problem\n",
    "    \"\"\"\n",
    "    def __init__(self, od_matrices, capacities, free_flow_times, link_flows):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            od_matrices: List/array of OD demand matrices [num_samples, num_nodes, num_nodes]\n",
    "            capacities: List/array of capacity matrices [num_samples, num_edges]\n",
    "            free_flow_times: List/array of free-flow time matrices [num_samples, num_nodes, num_nodes]\n",
    "            link_flows: List/array of ground truth link flows [num_samples, num_edges]\n",
    "        \"\"\"\n",
    "        self.od_matrices = torch.tensor(od_matrices, dtype=torch.float32)\n",
    "        self.capacities = torch.tensor(capacities, dtype=torch.float32)\n",
    "        self.free_flow_times = torch.tensor(free_flow_times, dtype=torch.float32)\n",
    "        self.link_flows = torch.tensor(link_flows, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.od_matrices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'od_matrix': self.od_matrices[idx],\n",
    "            'capacity': self.capacities[idx],\n",
    "            'free_flow_time': self.free_flow_times[idx],\n",
    "            'link_flows': self.link_flows[idx]\n",
    "        }\n",
    "    \n",
    "    def get_batches(self, batch_size):\n",
    "        dataloader = torch.utils.data.DataLoader(\n",
    "            self, batch_size=batch_size, shuffle=True\n",
    "        )\n",
    "        return dataloader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kan_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
