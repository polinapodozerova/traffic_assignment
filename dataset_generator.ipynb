{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56aca9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "from utils import create_network_df, generate_od_matrices, prepare_network_data, generate_capacity_matrices\n",
    "from leblanc import leblanc_algorithm\n",
    "from tqdm import tqdm\n",
    "import openmatrix as omx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1ff4cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath = 'data/sioux'\n",
    "filepath = 'data/ema'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3df03539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sioux = create_network_df(network_name=\"SiouxFalls\")\n",
    "# T_0, C = prepare_network_data(sioux)\n",
    "# eps = 0.005\n",
    "ema = create_network_df(network_name=\"EMA\")\n",
    "T_0, C = prepare_network_data(ema)\n",
    "eps = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20d02cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D = np.array([\n",
    "#     [0, 100, 100, 500, 200, 300, 500, 800, 500, 1300, 500, 200, 500, 300, 500, 500, 400, 100, 300, 300, 100, 400, 300, 100],\n",
    "#     [100, 0, 100, 200, 100, 400, 200, 400, 200, 600, 200, 100, 300, 100, 100, 400, 200, 0, 100, 100, 0, 100, 0, 0],\n",
    "#     [100, 100, 0, 200, 100, 300, 100, 200, 100, 300, 300, 200, 100, 100, 100, 200, 100, 0, 0, 0, 0, 100, 100, 0],\n",
    "#     [500, 200, 200, 0, 500, 400, 400, 700, 700, 1200, 1400, 600, 600, 500, 500, 800, 500, 100, 200, 300, 200, 400, 500, 200],\n",
    "#     [200, 100, 100, 500, 0, 200, 200, 500, 800, 1000, 500, 200, 200, 100, 200, 500, 200, 0, 100, 100, 100, 200, 100, 0],\n",
    "#     [300, 400, 300, 400, 200, 0, 400, 800, 400, 800, 400, 200, 200, 100, 200, 900, 500, 100, 200, 300, 100, 200, 100, 100],\n",
    "#     [500, 200, 100, 400, 200, 400, 0, 1000, 600, 1900, 500, 700, 400, 200, 500, 1400, 1000, 200, 400, 500, 200, 500, 200, 100],\n",
    "#     [800, 400, 200, 700, 500, 800, 1000, 0, 800, 1600, 800, 600, 600, 400, 600, 2200, 1400, 300, 700, 900, 400, 500, 300, 200],\n",
    "#     [500, 200, 100, 700, 800, 400, 600, 800, 0, 2800, 1400, 600, 600, 600, 900, 1400, 900, 200, 400, 600, 300, 700, 500, 200],\n",
    "#     [1300, 600, 300, 1200, 1000, 800, 1900, 1600, 2800, 0, 4000, 2000, 1900, 2100, 4000, 4400, 3900, 700, 1800, 2500, 1200, 2600, 1800, 800],\n",
    "#     [500, 200, 300, 1500, 500, 400, 500, 800, 1400, 3900, 0, 1400, 1000, 1600, 1400, 1400, 1000, 100, 400, 600, 400, 1100, 1300, 600],\n",
    "#     [200, 100, 200, 600, 200, 200, 700, 600, 600, 2000, 1400, 0, 1300, 700, 700, 700, 600, 200, 300, 400, 300, 700, 700, 500],\n",
    "#     [500, 300, 100, 600, 200, 200, 400, 600, 600, 1900, 1000, 1300, 0, 600, 700, 600, 500, 100, 300, 600, 600, 1300, 800, 800],\n",
    "#     [300, 100, 100, 500, 100, 100, 200, 400, 600, 2100, 1600, 700, 600, 0, 1300, 700, 700, 100, 300, 500, 400, 1200, 1100, 400],\n",
    "#     [500, 100, 100, 500, 200, 200, 500, 600, 1000, 4000, 1400, 700, 700, 1300, 0, 1200, 1500, 200, 800, 1100, 800, 2600, 1000, 400],\n",
    "#     [500, 400, 200, 800, 500, 900, 1400, 2200, 1400, 4400, 1400, 700, 600, 700, 1200, 0, 2800, 500, 1300, 1600, 600, 1200, 500, 300],\n",
    "#     [400, 200, 100, 500, 200, 500, 1000, 1400, 900, 3900, 1000, 600, 500, 700, 1500, 2800, 0, 600, 1700, 1700, 600, 1700, 600, 300],\n",
    "#     [100, 0, 0, 100, 0, 100, 200, 300, 200, 700, 200, 200, 100, 100, 200, 500, 600, 0, 300, 400, 100, 300, 100, 0],\n",
    "#     [300, 100, 0, 200, 100, 200, 400, 700, 400, 1800, 400, 300, 300, 300, 800, 1300, 1700, 300, 0, 1200, 400, 1200, 300, 100],\n",
    "#     [300, 100, 0, 300, 100, 300, 500, 900, 600, 2500, 600, 500, 600, 500, 1100, 1600, 1700, 400, 1200, 0, 1200, 2400, 700, 400],\n",
    "#     [100, 0, 0, 200, 100, 100, 200, 400, 300, 1200, 400, 300, 600, 400, 800, 600, 600, 100, 400, 1200, 0, 1800, 700, 500],\n",
    "#     [400, 100, 100, 400, 200, 200, 500, 500, 700, 2600, 1100, 700, 1300, 1200, 2600, 1200, 1700, 300, 1200, 2400, 1800, 0, 2100, 1100],\n",
    "#     [300, 0, 100, 500, 100, 100, 200, 300, 500, 1800, 1300, 700, 800, 1100, 1000, 500, 600, 100, 300, 700, 700, 2100, 0, 700],\n",
    "#     [100, 0, 0, 200, 0, 100, 100, 200, 200, 800, 600, 500, 700, 400, 400, 300, 300, 0, 100, 400, 500, 1100, 700, 0]\n",
    "# ])\n",
    "def read_omx_demand(file_path):\n",
    "    try:\n",
    "        with omx.open_file(file_path, 'r') as omx_file:\n",
    "            matrix_names = omx_file.list_matrices()\n",
    "            \n",
    "            if not matrix_names:\n",
    "                raise ValueError(\"No matrices found in OMX file\")\n",
    "            \n",
    "            matrix_name = matrix_names[0]\n",
    "            demand_matrix = omx_file[matrix_name]\n",
    "            \n",
    "            mapping_title = 'NO_TITLE'\n",
    "            if hasattr(omx_file, 'mappings'):\n",
    "                mapping_dict = omx_file.mappings()\n",
    "                if mapping_dict:\n",
    "                    mapping_title = next(iter(mapping_dict.keys()))\n",
    "            \n",
    "            try:\n",
    "                lookup = omx_file.mapping(title=mapping_title)\n",
    "                zones = list(lookup.values())\n",
    "            except:\n",
    "                zones = list(range(1, demand_matrix.shape[0] + 1))\n",
    "                lookup = {zone: idx for idx, zone in enumerate(zones, 1)}\n",
    "            \n",
    "            return {\n",
    "                'matrix': np.array(demand_matrix),\n",
    "                'zones': zones,\n",
    "                'lookup': lookup\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading OMX file: {e}\")\n",
    "        raise\n",
    "\n",
    "demand_data = read_omx_demand(\"/home/polina/kans/traffic_assignment/TransportationNetworks/_scripts/demand.omx\")\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    demand_data['matrix'],\n",
    "    index=demand_data['zones'],\n",
    "    columns=demand_data['zones']\n",
    ")\n",
    "\n",
    "D = np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8a94de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(filepath + '/uncongested', exist_ok=True)\n",
    "os.makedirs(filepath + '/moderate', exist_ok=True)\n",
    "os.makedirs(filepath + '/congested', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7556b45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_matrices=5000\n",
    "uncongested_matrices = generate_od_matrices(D, num_matrices, 'uncongested')\n",
    "capacities = generate_capacity_matrices(C, num_matrices, disruption_level='L')\n",
    "# moderate_matrices = generate_od_matrices(D, num_matrices, 'moderate')\n",
    "# congested_matrices = generate_od_matrices(D, num_matrices, 'congested')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a7da8e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2a5c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_file(condition, iters, times, eps, filepath='data/EMA'):\n",
    "    files = os.listdir(filepath + f\"/{condition}\")\n",
    "    indices = [int(f.split('_')[1].split('.')[0]) for f in files if f.startswith('sample_')]\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'condition': condition,\n",
    "        'eps' : eps,\n",
    "        'sample_id': indices,\n",
    "        'iterations' : iters,\n",
    "        'execution_time' : times,\n",
    "        'filename': [filepath + f\"/{condition}/sample_{i:04d}.pkl\" for i in indices]\n",
    "    })\n",
    "    df.to_csv(filepath + f\"/{condition}_index.csv\", index=False)\n",
    "\n",
    "def save_dataset(condition='uncongested', num_samples=5000):\n",
    "    start_time = time.time()\n",
    "    od_matrices = generate_od_matrices(D, num_samples, condition)\n",
    "    capacities = generate_capacity_matrices(C, num_samples, disruption_level='L')\n",
    "    times = []\n",
    "    iters = []\n",
    "    for i in tqdm(range(num_samples), desc=f\"Обработка {condition}\"):\n",
    "        result_matrix, iter_num = leblanc_algorithm(T_0, od_matrices[i], capacities[i], eps)\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        times.append(total_time)\n",
    "        iters.append(iter_num)\n",
    "        metadata = {\n",
    "            'iterations': iter_num,\n",
    "            'execution_time': total_time,\n",
    "        }\n",
    "        data_pair = {\n",
    "            'input': od_matrices[i],\n",
    "            'output': result_matrix,\n",
    "            'metadata' : metadata\n",
    "        }\n",
    "        \n",
    "        filename = filepath + f\"/{condition}/sample_{i:04d}.pkl\"\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(data_pair, f)\n",
    "\n",
    "    create_index_file(condition, iters, times, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21514da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обработка uncongested:   0%|          | 3/1000 [01:03<5:50:06, 21.07s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43msave_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36msave_dataset\u001b[39m\u001b[34m(condition, num_samples)\u001b[39m\n\u001b[32m     20\u001b[39m iters = []\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(num_samples), desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mОбработка \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcondition\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     result_matrix, iter_num = \u001b[43mleblanc_algorithm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mT_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mod_matrices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapacities\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m     end_time = time.time()\n\u001b[32m     24\u001b[39m     total_time = end_time - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/kans/traffic_assignment/leblanc.py:32\u001b[39m, in \u001b[36mleblanc_algorithm\u001b[39m\u001b[34m(T_0, D, C, epsilon)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Шаг 2: Расчет нового потока\u001b[39;00m\n\u001b[32m     31\u001b[39m new_graph = nx.from_numpy_array(T, create_using=nx.DiGraph)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m Y = \u001b[43mget_flow_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Шаг 3: Поиск оптимального шага (lambda)\u001b[39;00m\n\u001b[32m     35\u001b[39m C_inv_4 = \u001b[32m1\u001b[39m / C**\u001b[32m4\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/kans/traffic_assignment/leblanc.py:71\u001b[39m, in \u001b[36mget_flow_matrix\u001b[39m\u001b[34m(G, n, D)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i != j \u001b[38;5;129;01mand\u001b[39;00m D[i, j] != \u001b[32m0\u001b[39m:\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m         path = \u001b[43mnx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshortest_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mweight\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(path)-\u001b[32m1\u001b[39m):\n\u001b[32m     73\u001b[39m             X[path[k], path[k+\u001b[32m1\u001b[39m]] += D[i, j]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<class 'networkx.utils.decorators.argmap'> compilation 16:3\u001b[39m, in \u001b[36margmap_shortest_path_13\u001b[39m\u001b[34m(G, source, target, weight, method, backend, **backend_kwargs)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbz2\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgzip\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01minspect\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mitertools\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/kans/traffic_assignment/.venv/lib/python3.12/site-packages/networkx/utils/backends.py:967\u001b[39m, in \u001b[36m_dispatchable.__call__\u001b[39m\u001b[34m(self, backend, *args, **kwargs)\u001b[39m\n\u001b[32m    965\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m backend != \u001b[33m\"\u001b[39m\u001b[33mnetworkx\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    966\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m backend is not installed\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m967\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43morig_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[38;5;66;03m# Use `backend_name` in this function instead of `backend`.\u001b[39;00m\n\u001b[32m    970\u001b[39m \u001b[38;5;66;03m# This is purely for aesthetics and to make it easier to search for this\u001b[39;00m\n\u001b[32m    971\u001b[39m \u001b[38;5;66;03m# variable since \"backend\" is used in many comments and log/error messages.\u001b[39;00m\n\u001b[32m    972\u001b[39m backend_name = backend\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/kans/traffic_assignment/.venv/lib/python3.12/site-packages/networkx/algorithms/shortest_paths/generic.py:185\u001b[39m, in \u001b[36mshortest_path\u001b[39m\u001b[34m(G, source, target, weight, method)\u001b[39m\n\u001b[32m    183\u001b[39m     paths = nx.bidirectional_shortest_path(G, source, target)\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m method == \u001b[33m\"\u001b[39m\u001b[33mdijkstra\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     _, paths = \u001b[43mnx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbidirectional_dijkstra\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# method == 'bellman-ford':\u001b[39;00m\n\u001b[32m    187\u001b[39m     paths = nx.bellman_ford_path(G, source, target, weight)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<class 'networkx.utils.decorators.argmap'> compilation 20:3\u001b[39m, in \u001b[36margmap_bidirectional_dijkstra_17\u001b[39m\u001b[34m(G, source, target, weight, backend, **backend_kwargs)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbz2\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgzip\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01minspect\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mitertools\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/kans/traffic_assignment/.venv/lib/python3.12/site-packages/networkx/utils/backends.py:967\u001b[39m, in \u001b[36m_dispatchable.__call__\u001b[39m\u001b[34m(self, backend, *args, **kwargs)\u001b[39m\n\u001b[32m    965\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m backend != \u001b[33m\"\u001b[39m\u001b[33mnetworkx\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    966\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m backend is not installed\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m967\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43morig_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[38;5;66;03m# Use `backend_name` in this function instead of `backend`.\u001b[39;00m\n\u001b[32m    970\u001b[39m \u001b[38;5;66;03m# This is purely for aesthetics and to make it easier to search for this\u001b[39;00m\n\u001b[32m    971\u001b[39m \u001b[38;5;66;03m# variable since \"backend\" is used in many comments and log/error messages.\u001b[39;00m\n\u001b[32m    972\u001b[39m backend_name = backend\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/kans/traffic_assignment/.venv/lib/python3.12/site-packages/networkx/algorithms/shortest_paths/weighted.py:2416\u001b[39m, in \u001b[36mbidirectional_dijkstra\u001b[39m\u001b[34m(G, source, target, weight)\u001b[39m\n\u001b[32m   2412\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (finaldist, finalpath)\n\u001b[32m   2414\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m w, d \u001b[38;5;129;01min\u001b[39;00m neighs[\u001b[38;5;28mdir\u001b[39m][v].items():\n\u001b[32m   2415\u001b[39m     \u001b[38;5;66;03m# weight(v, w, d) for forward and weight(w, v, d) for back direction\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2416\u001b[39m     cost = weight(v, w, d) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mdir\u001b[39m == \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mweight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2417\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m cost \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2418\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/kans/traffic_assignment/.venv/lib/python3.12/site-packages/networkx/algorithms/shortest_paths/weighted.py:78\u001b[39m, in \u001b[36m_weight_function.<locals>.<lambda>\u001b[39m\u001b[34m(u, v, data)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m G.is_multigraph():\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m u, v, d: \u001b[38;5;28mmin\u001b[39m(attr.get(weight, \u001b[32m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m d.values())\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m u, v, data: data.get(weight, \u001b[32m1\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "save_dataset(num_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484e02df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обработка moderate:   0%|          | 0/1000 [00:00<?, ?it/s]/home/polina/kans/leblanc.py:27: RuntimeWarning: invalid value encountered in divide\n",
      "  T = T_0 * (1 + 0.15 * (X / C)**4)\n",
      "/home/polina/kans/leblanc.py:35: RuntimeWarning: divide by zero encountered in divide\n",
      "  C_inv_4 = 1 / C**4\n",
      "Обработка moderate: 100%|██████████| 1000/1000 [29:53<00:00,  1.79s/it]\n"
     ]
    }
   ],
   "source": [
    "save_dataset(condition='moderate', num_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e35a7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обработка congested:  49%|████▊     | 487/1000 [30:08<35:01,  4.10s/it] "
     ]
    }
   ],
   "source": [
    "save_dataset(condition='congested', num_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e47d183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting aiofiles\n",
      "  Using cached aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Using cached aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: aiofiles\n",
      "Successfully installed aiofiles-24.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install aiofiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5ded39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiofiles\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def save_sample(condition, i, od_matrix, capacity, eps, filepath):\n",
    "    \"\"\"Async function to process and save a single sample\"\"\"\n",
    "    start_time = time.time()\n",
    "    result_matrix, iter_num = leblanc_algorithm(T_0, od_matrix, capacity, eps)\n",
    "    exec_time = time.time() - start_time\n",
    "    \n",
    "    metadata = {\n",
    "        'iterations': iter_num,\n",
    "        'execution_time': exec_time,\n",
    "    }\n",
    "    data_pair = {\n",
    "        'input': od_matrix,\n",
    "        'output': result_matrix,\n",
    "        'metadata': metadata\n",
    "    }\n",
    "    \n",
    "    filename = f\"{filepath}/{condition}/sample_{i:04d}.pkl\"\n",
    "    async with aiofiles.open(filename, 'wb') as f:\n",
    "        await f.write(pickle.dumps(data_pair))\n",
    "    \n",
    "    return i, iter_num, exec_time\n",
    "\n",
    "async def process_batch(condition, batch_od, batch_c, eps, filepath, start_idx):\n",
    "    \"\"\"Process a batch of samples asynchronously\"\"\"\n",
    "    tasks = [\n",
    "        save_sample(condition, start_idx+i, od, cap, eps, filepath)\n",
    "        for i, (od, cap) in enumerate(zip(batch_od, batch_c))\n",
    "    ]\n",
    "    return await tqdm_asyncio.gather(*tasks, desc=f\"Processing {condition} batch\")\n",
    "\n",
    "async def save_dataset_async(condition='uncongested', num_samples=5000, batch_size=100, eps=0.005, filepath='data/EMA'):\n",
    "    \"\"\"Main async function to generate and save dataset\"\"\"\n",
    "    start_total = time.time()\n",
    "    \n",
    "    # Create directory if not exists\n",
    "    os.makedirs(f\"{filepath}/{condition}\", exist_ok=True)\n",
    "    \n",
    "    # Generate all matrices upfront\n",
    "    od_matrices = generate_od_matrices(D, num_samples, condition)\n",
    "    capacities = generate_capacity_matrices(C, num_samples, disruption_level='L')\n",
    "    \n",
    "    # Process in batches\n",
    "    all_results = []\n",
    "    for batch_start in range(0, num_samples, batch_size):\n",
    "        batch_end = min(batch_start + batch_size, num_samples)\n",
    "        batch_results = await process_batch(\n",
    "            condition,\n",
    "            od_matrices[batch_start:batch_end],\n",
    "            capacities[batch_start:batch_end],\n",
    "            eps,\n",
    "            filepath,\n",
    "            batch_start\n",
    "        )\n",
    "        all_results.extend(batch_results)\n",
    "    \n",
    "    # Prepare and save index file\n",
    "    indices, iters, times = zip(*all_results)\n",
    "    create_index_file(condition, iters, times, eps, filepath)\n",
    "    \n",
    "    print(f\"Total processing time: {time.time() - start_total:.2f} seconds\")\n",
    "\n",
    "def create_index_file(condition, iters, times, eps, filepath='data/EMA'):\n",
    "    \"\"\"Sync function to create index file (unchanged)\"\"\"\n",
    "    df = pd.DataFrame({\n",
    "        'condition': condition,\n",
    "        'eps': eps,\n",
    "        'sample_id': range(len(iters)),\n",
    "        'iterations': iters,\n",
    "        'execution_time': times,\n",
    "        'filename': [f\"{filepath}/{condition}/sample_{i:04d}.pkl\" for i in range(len(iters))]\n",
    "    })\n",
    "    df.to_csv(f\"{filepath}/{condition}_index.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f299deb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available cores: 12\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "print(f\"Available cores: {multiprocessing.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a4c8f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/polina/kans/traffic_assignment/leblanc.py:27: RuntimeWarning: invalid value encountered in divide\n",
      "  T = T_0 * (1 + 0.15 * (X / C)**4)\n",
      "/home/polina/kans/traffic_assignment/leblanc.py:27: RuntimeWarning: invalid value encountered in divide\n",
      "  T = T_0 * (1 + 0.15 * (X / C)**4)\n",
      "Processing:   0%|          | 0/5000 [00:00<?, ?it/s]/home/polina/kans/traffic_assignment/leblanc.py:27: RuntimeWarning: invalid value encountered in divide\n",
      "  T = T_0 * (1 + 0.15 * (X / C)**4)\n",
      "/home/polina/kans/traffic_assignment/leblanc.py:35: RuntimeWarning: divide by zero encountered in divide\n",
      "  C_inv_4 = 1 / C**4\n",
      "/home/polina/kans/traffic_assignment/leblanc.py:27: RuntimeWarning: invalid value encountered in divide\n",
      "  T = T_0 * (1 + 0.15 * (X / C)**4)\n",
      "/home/polina/kans/traffic_assignment/leblanc.py:27: RuntimeWarning: invalid value encountered in divide\n",
      "  T = T_0 * (1 + 0.15 * (X / C)**4)\n",
      "/home/polina/kans/traffic_assignment/leblanc.py:35: RuntimeWarning: divide by zero encountered in divide\n",
      "  C_inv_4 = 1 / C**4\n",
      "/home/polina/kans/traffic_assignment/leblanc.py:27: RuntimeWarning: invalid value encountered in divide\n",
      "  T = T_0 * (1 + 0.15 * (X / C)**4)\n",
      "/home/polina/kans/traffic_assignment/leblanc.py:27: RuntimeWarning: invalid value encountered in divide\n",
      "  T = T_0 * (1 + 0.15 * (X / C)**4)\n",
      "/home/polina/kans/traffic_assignment/leblanc.py:27: RuntimeWarning: invalid value encountered in divide\n",
      "  T = T_0 * (1 + 0.15 * (X / C)**4)\n",
      "/home/polina/kans/traffic_assignment/leblanc.py:27: RuntimeWarning: invalid value encountered in divide\n",
      "  T = T_0 * (1 + 0.15 * (X / C)**4)\n",
      "/home/polina/kans/traffic_assignment/leblanc.py:27: RuntimeWarning: invalid value encountered in divide\n",
      "  T = T_0 * (1 + 0.15 * (X / C)**4)\n",
      "/home/polina/kans/traffic_assignment/leblanc.py:35: RuntimeWarning: divide by zero encountered in divide\n",
      "  C_inv_4 = 1 / C**4\n",
      "/home/polina/kans/traffic_assignment/leblanc.py:35: RuntimeWarning: divide by zero encountered in divide\n",
      "  C_inv_4 = 1 / C**4\n",
      "/home/polina/kans/traffic_assignment/leblanc.py:35: RuntimeWarning: divide by zero encountered in divide\n",
      "  C_inv_4 = 1 / C**4\n",
      "/home/polina/kans/traffic_assignment/leblanc.py:35: RuntimeWarning: divide by zero encountered in divide\n",
      "  C_inv_4 = 1 / C**4\n",
      "/home/polina/kans/traffic_assignment/leblanc.py:35: RuntimeWarning: divide by zero encountered in divide\n",
      "  C_inv_4 = 1 / C**4\n",
      "/home/polina/kans/traffic_assignment/leblanc.py:35: RuntimeWarning: divide by zero encountered in divide\n",
      "  C_inv_4 = 1 / C**4\n",
      "/home/polina/kans/traffic_assignment/leblanc.py:35: RuntimeWarning: divide by zero encountered in divide\n",
      "  C_inv_4 = 1 / C**4\n",
      "/home/polina/kans/traffic_assignment/leblanc.py:35: RuntimeWarning: divide by zero encountered in divide\n",
      "  C_inv_4 = 1 / C**4\n",
      "Processing: 100%|██████████| 5000/5000 [44:48<00:00,  1.86it/s, completed=5000/5000, avg_time=5.45s, remaining=0.0min, total_est=44.8min] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed 5000 samples in 44.8 minutes\n",
      "Average time per sample: 0.54s\n",
      "Throughput: 111.5 samples/minute\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "\n",
    "def process_sample(args):\n",
    "    i, od, cap, condition, eps, filepath = args\n",
    "    start_time = time.time()\n",
    "    result_matrix, iter_num = leblanc_algorithm(T_0, od, cap, eps)\n",
    "    exec_time = time.time() - start_time\n",
    "    \n",
    "    data = {\n",
    "        'input': od,\n",
    "        'output': result_matrix,\n",
    "        'metadata': {\n",
    "            'iterations': iter_num,\n",
    "            'execution_time': exec_time\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(f\"{filepath}/{condition}/sample_{i:04d}.pkl\", 'wb') as f:\n",
    "        pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    return i, iter_num, exec_time\n",
    "\n",
    "def save_dataset_fast(condition='uncongested', num_samples=5000, filepath='data/EMA', eps=0.05):\n",
    "    start_total = time.time()\n",
    "    os.makedirs(f\"{filepath}/{condition}\", exist_ok=True)\n",
    "    \n",
    "    od_matrices = generate_od_matrices(D, num_samples, condition)\n",
    "    capacities = generate_capacity_matrices(C, num_samples, disruption_level='L')\n",
    "    \n",
    "    args = [(i, od_matrices[i], capacities[i], condition, eps, filepath) for i in range(num_samples)]\n",
    "    \n",
    "    completed = 0\n",
    "    time_per_sample = []\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=10) as executor:\n",
    "        futures = {executor.submit(process_sample, arg): i for i, arg in enumerate(args)}\n",
    "        \n",
    "        with tqdm(total=num_samples, desc=\"Processing\") as pbar:\n",
    "            for future in as_completed(futures):\n",
    "                i, iter_num, exec_time = future.result()\n",
    "                time_per_sample.append(exec_time)\n",
    "                completed += 1\n",
    "                \n",
    "                avg_time = np.mean(time_per_sample[-100:]) if len(time_per_sample) > 0 else 0\n",
    "                remaining = (num_samples - completed) * avg_time / 12\n",
    "                \n",
    "                pbar.set_postfix({\n",
    "                    'completed': f\"{completed}/{num_samples}\",\n",
    "                    'avg_time': f\"{avg_time:.2f}s\",\n",
    "                    'remaining': f\"{remaining/60:.1f}min\",\n",
    "                    'total_est': f\"{(time.time() - start_total + remaining)/60:.1f}min\"\n",
    "                })\n",
    "                pbar.update(1)\n",
    "    \n",
    "    results = sorted([future.result() for future in futures], key=lambda x: x[0])\n",
    "    indices, iters, times = zip(*results)\n",
    "    \n",
    "    pd.DataFrame({\n",
    "        'condition': condition,\n",
    "        'eps': eps,\n",
    "        'sample_id': indices,\n",
    "        'iterations': iters,\n",
    "        'execution_time': times,\n",
    "        'filename': [f\"{filepath}/{condition}/sample_{i:04d}.pkl\" for i in indices]\n",
    "    }).to_csv(f\"{filepath}/{condition}_index.csv\", index=False)\n",
    "    \n",
    "    total_time = time.time() - start_total\n",
    "    print(f\"\\nCompleted {num_samples} samples in {total_time/60:.1f} minutes\")\n",
    "    print(f\"Average time per sample: {total_time/num_samples:.2f}s\")\n",
    "    print(f\"Throughput: {num_samples/total_time*60:.1f} samples/minute\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    save_dataset_fast(num_samples=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c01fb95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = generate_od_matrices(D, 1)[0], capacities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32566898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[   0,    0,  707, ...,    0,    0,    0],\n",
       "        [   0,    0, 1323, ...,    0,    0,    0],\n",
       "        [ 297, 1091,    0, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [   0,    0,    0, ...,    0,    0,    0],\n",
       "        [   0,    0,    0, ...,    0,    0,    0],\n",
       "        [   0,    0,    0, ...,    0,    0,    0]], shape=(74, 74)),\n",
       " 28)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leblanc_algorithm(T_0, x, y, epsilon=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1f0d2d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[   0,    0,  707, ...,    0,    0,    0],\n",
       "        [   0,    0, 1323, ...,    0,    0,    0],\n",
       "        [ 297, 1091,    0, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [   0,    0,    0, ...,    0,    0,    0],\n",
       "        [   0,    0,    0, ...,    0,    0,    0],\n",
       "        [   0,    0,    0, ...,    0,    0,    0]], shape=(74, 74)),\n",
       " 189)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leblanc_algorithm(T_0, x, y, epsilon=0.005)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
